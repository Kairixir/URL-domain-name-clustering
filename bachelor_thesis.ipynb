{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edcdc741-3f08-4f14-9dcb-fee35adf17b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# URL clustering based on similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a4ea9e-cbfd-40bf-8edf-1e5f7d0c249a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports & Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099e27e-3bc4-4243-866d-9fd3a6c9acd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save or load jupyter session"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52be716f-a093-4e01-9436-092fcaef2232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T16:57:20.270196Z",
     "iopub.status.busy": "2023-04-28T16:57:20.269345Z",
     "iopub.status.idle": "2023-04-28T16:57:21.985755Z",
     "shell.execute_reply": "2023-04-28T16:57:21.976144Z",
     "shell.execute_reply.started": "2023-04-28T16:57:20.270156Z"
    },
    "tags": []
   },
   "source": [
    "# Save\n",
    "import datetime\n",
    "\n",
    "import dill\n",
    "\n",
    "SESSION_DIR = \"jupyter_sessions\"\n",
    "PREFIX = datetime.datetime.now()\n",
    "SUFFIX = \"\"\n",
    "\n",
    "file_path = (\n",
    "    f'{SESSION_DIR}/{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}{SUFFIX}.db'\n",
    ")\n",
    "dill.detect.trace(False)\n",
    "dill.dump_session(file_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aef9c046-8396-4084-9add-2ccc04f470f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T09:21:18.349024Z",
     "iopub.status.busy": "2023-04-15T09:21:18.348591Z",
     "iopub.status.idle": "2023-04-15T09:21:25.275127Z",
     "shell.execute_reply": "2023-04-15T09:21:25.273506Z",
     "shell.execute_reply.started": "2023-04-15T09:21:18.348992Z"
    },
    "tags": []
   },
   "source": [
    "# Load\n",
    "import dill\n",
    "\n",
    "SESSION_DIR = \"jupyter_sessions\"\n",
    "FILE_NAME = \"20230414203732\"\n",
    "\n",
    "dill.load_session(f\"{SESSION_DIR}/{FILE_NAME}.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b302250-f8da-4335-9c4a-41b1fff31c4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7329734-9b2a-41ec-ba43-4c2246c6028b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "import smaz\n",
    "import tldextract\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import Hashingvectoriser, TfidfTransformer\n",
    "from sklearn.metrics import (\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e78c63-2f12-42c7-be48-27906861093c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c61a8-132e-4b0f-bf8e-ca63b096e306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load input data\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter=\",\",\n",
    "        dtype={\"url\": \"string\"},\n",
    "        usecols=[\"url\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b7478-9311-4893-bd02-7e21da195ed0",
   "metadata": {},
   "source": [
    "## Non TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85f553-cbc3-4eec-b4a4-c7466f0f6351",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data extraction & cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adb08b-bb91-471f-bcf2-d1a92e264613",
   "metadata": {},
   "source": [
    "#### Extract Fully Qualified Domain Names (FQDNs) from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb3157-4c8f-4ac1-994b-24da10c682a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_FQDNs(df):\n",
    "    \"\"\"\n",
    "    Extract FQDNs from URLs\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Regex pattern to extract fully qualified domain name (FQDN)\n",
    "    pattern = r\"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?(?P<domain>[\\w\\.\\-]+)[^\\w]*\"\n",
    "\n",
    "    # Execute regex over URLs\n",
    "    match = df[\"url\"].str.extract(pattern)\n",
    "\n",
    "    # Extract domain using named group\n",
    "    df[\"FQDN\"] = match[\"domain\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa406c1f-e1d6-4b6a-a74a-f920b325d65d",
   "metadata": {},
   "source": [
    "#### Remove all FQDN duplicates and addreses IPv4 addresses\n",
    "\n",
    "IPv6 are cleaned further down in `clean_invalid_domains(df)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf3dca-9569-4dd8-8753-40e2070b994d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean data of FQDN duplicates and IPv4 addresses\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Pattern that matches all IPv4 addresses\n",
    "    pattern = \"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}([/:].*)?$\"\n",
    "\n",
    "    # Leave only data not containing pure IPv4\n",
    "    df = df[~df[\"url\"].str.match(pattern)]\n",
    "\n",
    "    # Remove all data with non-unique FQDN\n",
    "    df = df.drop_duplicates(subset=\"FQDN\")\n",
    "\n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71886ded-a5f4-4d64-aad8-f007a6e52056",
   "metadata": {},
   "source": [
    "#### Separate TLD, domain and subdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729e738-9a2c-4b55-a736-4d6914996182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_url_components(url):\n",
    "    \"\"\"\n",
    "    Extract relevant components of FQDN from URL using tldextract\n",
    "        Get information about validity of FQDN through suffix and tldextract (using The Public Suffix List)\n",
    "    \"\"\"\n",
    "    ext = tldextract.extract(url)\n",
    "    return pd.Series(\n",
    "        [\n",
    "            ext.subdomain,\n",
    "            ext.domain,\n",
    "            ext.suffix,\n",
    "            ext.suffix == \"\",\n",
    "            ext.domain + \".\" + ext.suffix,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0ac75-2e2f-406a-ab69-d30e86021b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_urls(df):\n",
    "    \"\"\"\n",
    "    Get individual components of FQDNs from URLs\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Apply function to url column to extract domain components and explode into separate columns\n",
    "    df[[\"subdomain\", \"domain\", \"TLD\", \"is_invalid_TLD\", \"domainTLD\"]] = df[\"url\"].apply(\n",
    "        extract_url_components\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12e0b0-5436-4cbb-9f82-e2501ced55ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_invalid_domains(df):\n",
    "    \"\"\"\n",
    "    Remove all FQDNs with invalid TLD (ie. TLD not in Public Suffix List)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove domains with invalid TLD\n",
    "    df = df[~df[\"is_invalid_TLD\"]]\n",
    "\n",
    "    # Remove unused is_invalid_TLD\n",
    "    df = df.drop(\"is_invalid_TLD\", axis=1)\n",
    "\n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e55e0f-bbe2-4e48-ae4b-aaad5cbbd180",
   "metadata": {},
   "source": [
    "#### Length of domain, subdomain and TLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a7d8e-ade3-423b-8ca8-44b45211b27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_lengths(df):\n",
    "    \"\"\"\n",
    "    Extract lengths of individual FQDN components\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df[[\"domain_length\", \"subdomain_length\", \"TLD_length\"]] = df[\n",
    "        [\"domain\", \"subdomain\", \"TLD\"]\n",
    "    ].applymap(len)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a566b-c1e8-4c07-846f-aab069e6650c",
   "metadata": {},
   "source": [
    "#### Number of subdomains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29043be7-fdfb-4d27-8eba-2d9faf59258e",
   "metadata": {},
   "source": [
    "I decided to include www in the count of subdomains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9d70b-c3d1-4852-b36f-22b55eb9ccb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_subdomains_count(df):\n",
    "    \"\"\"\n",
    "    Extract number of subdomains (excluding Second-Level domain)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"num_of_subdomains\"] = (\n",
    "        df[\"subdomain\"].str.split(\".\").apply(lambda x: len(x) if x != [\"\"] else 0)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77a85a-a10a-46df-b0af-c5457763a742",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Characters frequency & vowel-to-consonant ratio\n",
    "Characters:\n",
    "- alphabetical - \"a-zA-Z\"\n",
    "- digits - \"0-9\"\n",
    "- special - all except alphabetical, digits and dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf9cce-7888-493e-a41d-b0f9efa82ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_char_frequency_vowel_consonant_ratio(df):\n",
    "    \"\"\"\n",
    "    Extract frequencies of character groups for each individual FQDN component\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    for column in [\"domain\", \"subdomain\", \"TLD\"]:\n",
    "        # Vowel-to-consonant ratio\n",
    "        vowel_counts = df[column].str.count(r\"[aeiouAEIOU]\")\n",
    "        consonant_counts = df[column].str.count(r\"[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z]\")\n",
    "\n",
    "        # Get alphabetical, numeric and special character counts for specific column\n",
    "        numeric_counts = df[column].str.count(r\"[0-9]\")\n",
    "        special_counts = df[column].str.count(r\"[^A-Za-z0-9\\s\\.]\")\n",
    "        alpha_counts = vowel_counts + consonant_counts\n",
    "\n",
    "        # Add them into DF\n",
    "        df[\n",
    "            [\n",
    "                f\"{column}_alpha_count\",\n",
    "                f\"{column}_numeric_count\",\n",
    "                f\"{column}_special_count\",\n",
    "                f\"{column}_vowel_consonant_ratio\",\n",
    "            ]\n",
    "        ] = pd.Series(\n",
    "            [\n",
    "                alpha_counts,\n",
    "                numeric_counts,\n",
    "                special_counts,\n",
    "                vowel_counts / consonant_counts,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90de86-c431-4d0d-b8e4-596d678bce9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Complexity of domain and subdomain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff41ad3-a441-4f0b-9bfe-567562ce3cea",
   "metadata": {},
   "source": [
    "Using compression algorithm (`smaz` python implementation) to approximate Kolmogorov complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ebd10c-e481-42ad-917f-d15009326d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approximate_kolmogorov_complexity(df):\n",
    "    \"\"\"\n",
    "    Approximate the Kolmogorov complexity of the 'FQDN' column in a DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Calculate the ratio of the length of the compressed string to the length of the original string\n",
    "    # This ratio is used as an approximation of the Kolmogorov complexity\n",
    "    df[[\"FQDN_complexity\"]] = df[[\"FQDN\"]].applymap(\n",
    "        lambda s: len(smaz.compress(s)) / len(s) if s != \"\" else np.nan\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbfe13-7f85-46d9-8d1b-2f3f0ced8089",
   "metadata": {},
   "source": [
    "#### Feature imputation, scaling & selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c91f6e-101c-4e77-bf30-a266a0cd1a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_values(df):\n",
    "    \"\"\"\n",
    "    Impute data for PCA application\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Replace NaN values with zeros\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Replace infinities with the maximum finite value in each column\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    max_values = df.max()\n",
    "\n",
    "    df.fillna(max_values, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445047aa-cea9-49e7-8124-9c67d78801ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slice_dataset(df, columns_to_take):\n",
    "    \"\"\"\n",
    "    Extract specified columns\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.loc[:, columns_to_take]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d667f1-824f-4f1c-887e-7dd3743d5e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardise_features(df):\n",
    "    \"\"\"\n",
    "    Standardise all features in df\n",
    "    \"\"\"\n",
    "\n",
    "    ssc = StandardScaler()\n",
    "\n",
    "    matrix = ssc.fit_transform(df)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc9984-5fdb-4511-99c0-6a916f484a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_features(matrix, random_state=None):\n",
    "    \"\"\"\n",
    "    Select best features from given matrix using PCA\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=0.95, random_state=random_state)\n",
    "    matrix = pca.fit_transform(matrix)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b61fc-0e6d-45e9-9ed0-715a57a21819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    \"\"\"\n",
    "    Extract all features from URLs\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df = extract_FQDNs(df)\n",
    "    df = clean_data(df)\n",
    "    df = expand_urls(df)\n",
    "    df = clean_invalid_domains(df)\n",
    "    df = extract_lengths(df)\n",
    "    df = extract_subdomains_count(df)\n",
    "    df = extract_char_frequency_vowel_consonant_ratio(df)\n",
    "    df = approximate_kolmogorov_complexity(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84463312-e47c-40c6-aa46-f3971fddc16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set all numeric features usable for PCA\n",
    "numeric_columns = [\n",
    "    \"domain_length\",\n",
    "    \"subdomain_length\",\n",
    "    \"TLD_length\",\n",
    "    \"num_of_subdomains\",\n",
    "    \"domain_alpha_count\",\n",
    "    \"domain_numeric_count\",\n",
    "    \"domain_special_count\",\n",
    "    \"domain_vowel_consonant_ratio\",\n",
    "    \"subdomain_alpha_count\",\n",
    "    \"subdomain_numeric_count\",\n",
    "    \"subdomain_special_count\",\n",
    "    \"subdomain_vowel_consonant_ratio\",\n",
    "    \"TLD_alpha_count\",\n",
    "    \"TLD_numeric_count\",\n",
    "    \"TLD_special_count\",\n",
    "    \"TLD_vowel_consonant_ratio\",\n",
    "    \"FQDN_complexity\",\n",
    "]\n",
    "\n",
    "\n",
    "def transform_features(df, numeric_columns=numeric_columns, random_state=None):\n",
    "    \"\"\"\n",
    "    Transform non-TF-IDF (non-sparse) features for BIRCH\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df = impute_values(df)\n",
    "    df = slice_dataset(df, numeric_columns)\n",
    "\n",
    "    # From here data is in np.ndarray\n",
    "    matrix = standardise_features(df)\n",
    "    matrix = select_features(matrix, random_state)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb44c8-67ba-41b6-9018-73373469ca58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fad5fa-194d-46c2-b89e-b4857989464a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Incremental wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7aa504-e0a1-4712-8c36-bc19b89cedd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IncrementalTfidf:\n",
    "    \"\"\"\n",
    "    A class to process text data in chunks and incrementally compute character-level TF-IDF values.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    hashing_vectoriser : Hashingvectoriser\n",
    "        vectoriser that converts text data to a term-document matrix using the hashing trick\n",
    "    tfidf_transformer : TfidfTransformer\n",
    "        Transformer that computes TF-IDF values from the term-document matrix\n",
    "    X_counts : scipy.sparse matrix\n",
    "        Accumulated term-document matrix\n",
    "    X_tfidf : scipy.sparse matrix\n",
    "        Accumulated TF-IDF representation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ngram_range=(1, 1),\n",
    "        n_features=2**20,\n",
    "        column_name=\"\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the IncrementalTfidf with a Hashingvectoriser and TfidfTransformer.\n",
    "        \"\"\"\n",
    "        self.hashing_vectoriser = Hashingvectoriser(\n",
    "            analyzer=\"char\",\n",
    "            token_pattern=None,\n",
    "            n_features=n_features,\n",
    "            ngram_range=ngram_range,\n",
    "        )\n",
    "        self.tfidf_transformer = TfidfTransformer()\n",
    "        self.X_counts = None\n",
    "        self.X_tfidf = None\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def update_tf_counts(self, chunk):\n",
    "        \"\"\"\n",
    "        Update the term-document matrix with the new chunk of text data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        chunk : pandas.Series or list of str\n",
    "            New chunk of text data\n",
    "        \"\"\"\n",
    "        # Transform the chunk of text data into a term-document matrix using the Hashingvectoriser\n",
    "        chunk_counts = self.hashing_vectoriser.transform(chunk)\n",
    "\n",
    "        # If this is the first chunk, set the term-document matrix to the transformed chunk\n",
    "        if self.X_counts is None:\n",
    "            self.X_counts = chunk_counts\n",
    "        else:\n",
    "            # Otherwise, stack the transformed chunk to the existing term-document matrix\n",
    "            self.X_counts = sp.vstack((self.X_counts, chunk_counts))\n",
    "\n",
    "    def update_idf(self):\n",
    "        \"\"\"\n",
    "        Update the TfidfTransformer based on the current term-document matrix.\n",
    "        \"\"\"\n",
    "        self.tfidf_transformer.fit(self.X_counts)\n",
    "\n",
    "    def partial_fit(self, chunk):\n",
    "        \"\"\"\n",
    "        Update the term-document matrix and fits the TfidfTransformer with the new chunk of text data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        chunk : pandas.Series or list of str\n",
    "            New chunk of text data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : IncrementalTfidf\n",
    "            The instance of the IncrementalTfidf\n",
    "        \"\"\"\n",
    "        self.update_tf_counts(chunk)\n",
    "        self.update_idf()\n",
    "        return self\n",
    "\n",
    "    def transform(self, chunk):\n",
    "        \"\"\"\n",
    "        Transform the given chunk of text data to a TF-IDF representation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        chunk : pandas.Series or list of str\n",
    "            Chunk of text data to be transformed\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        chunk_tfidf : scipy.sparse matrix\n",
    "            Transformed chunk in TF-IDF representation\n",
    "        \"\"\"\n",
    "        chunk_counts = self.hashing_vectoriser.transform(chunk)\n",
    "        chunk_tfidf = self.tfidf_transformer.transform(chunk_counts)\n",
    "        return chunk_tfidf\n",
    "\n",
    "    def partial_fit_transform(self, chunk):\n",
    "        \"\"\"\n",
    "        Update the term-document matrix, fit the TfidfTransformer, and transform the given chunk of text data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        chunk : pandas.Series or list of str\n",
    "            New chunk of text data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        chunk_tfidf : scipy.sparse matrix\n",
    "            Transformed chunk in TF-IDF representation\n",
    "        \"\"\"\n",
    "        self.partial_fit(chunk)\n",
    "        return self.transform(chunk)\n",
    "\n",
    "    def compute_tfidf(self):\n",
    "        \"\"\"\n",
    "        Retrieve the accumulated TF-IDF representation of the processed text data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_tfidf : scipy.sparse matrix\n",
    "            Accumulated TF-IDF representation\n",
    "        \"\"\"\n",
    "        self.X_tfidf = self.tfidf_transformer.transform(self.X_counts)\n",
    "        return self.X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d87f3e-d6ca-43f8-852d-2fa4571dfe0a",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e432c2c-b3c1-4a35-9877-d8a08552339e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_vectorisers(df, columns, ngram_ranges):\n",
    "    \"\"\"\n",
    "    Create TF-IDF vectorisers and fits them to specified columns\n",
    "    \"\"\"\n",
    "    fitted_tfidf_vectorisers = []\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        incremental_tfidf = IncrementalTfidf(\n",
    "            ngram_range=ngram_ranges[i], column_name=column\n",
    "        )\n",
    "\n",
    "        incremental_tfidf.partial_fit(df[column])\n",
    "\n",
    "        fitted_tfidf_vectorisers.append(incremental_tfidf)\n",
    "\n",
    "    return fitted_tfidf_vectorisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64605a7-c033-4ecb-9ff3-bf945a82225d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def increment_fit_vectorisers(df, fitted_vectorisers):\n",
    "    \"\"\"\n",
    "    Update TF-IDF vectorisers incrementally\n",
    "    \"\"\"\n",
    "\n",
    "    for vectoriser in fitted_vectorisers:\n",
    "        vectoriser.partial_fit(df[vectoriser.column_name])\n",
    "\n",
    "    return fitted_vectorisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86baea-613a-4300-aa21-f2fa35dd13fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tfidf_matrices(fitted_vectorisers):\n",
    "    \"\"\"\n",
    "    Retrieve TF-IDF matrices from fitted vectorisers\n",
    "    \"\"\"\n",
    "\n",
    "    return [\n",
    "        incremental_tfidf.compute_tfidf() for incremental_tfidf in fitted_vectorisers\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39506bc3-f06a-4bd5-bc15-3bfea841ac95",
   "metadata": {},
   "source": [
    "### TF-IDF feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3443646-a9e0-4d08-9a93-d7c269779a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lsa_selection(tfidf_matrices, n_components, random_state):\n",
    "    \"\"\"\n",
    "    Perform Latent Semantic Analysis (LSA) on a list of TF-IDF matrices\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store the selected LSA-transformed TF-IDF matrices and their explained variances\n",
    "    selected_matrices = []\n",
    "    explained_variances = []\n",
    "\n",
    "    # Loop through each TF-IDF matrix\n",
    "    for i, tfidf_matrix in enumerate(tfidf_matrices):\n",
    "        # Perform LSA on the TF-IDF matrix\n",
    "        lsa = TruncatedSVD(n_components=n_components[i], random_state=random_state)\n",
    "\n",
    "        X_lsa = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Add the LSA-transformed TF-IDF matrix and its explained variance to their respective lists\n",
    "        selected_matrices.append(X_lsa)\n",
    "        explained_variances.append(lsa.explained_variance_ratio_.sum())\n",
    "\n",
    "    # Return the selected LSA-transformed TF-IDF matrices and their explained variances\n",
    "    return (selected_matrices, explained_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bb0de-5f77-45da-9e39-8fa5cc46c527",
   "metadata": {},
   "source": [
    "### TF-IDF preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89987c85-fb34-426b-af54-8f7495cb988f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_tfidf(df, columns, ngram_ranges, n_components, random_state=None):\n",
    "    \"\"\"\n",
    "    Preprocess data using TF-IDF vectorisation and Latent Semantic Analysis (LSA)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the TF-IDF vectorisers to the data in the specified columns and ngram ranges\n",
    "    fitted_vectorisers = fit_vectorisers(df, columns, ngram_ranges)\n",
    "\n",
    "    # Get the TF-IDF matrices from the fitted vectorisers\n",
    "    tfidf_matrices = get_tfidf_matrices(fitted_vectorisers)\n",
    "\n",
    "    # Perform LSA on the TF-IDF matrices\n",
    "    tfidf_lsas, explained_variances = lsa_selection(\n",
    "        tfidf_matrices, n_components, random_state\n",
    "    )\n",
    "\n",
    "    # Return the LSA-transformed TF-IDF matrices, their explained variances, and the fitted vectorisers\n",
    "    return tfidf_lsas, explained_variances, fitted_vectorisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2a113-5427-4c37-9e6b-803a8a3589ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def increment_preprocess_tfidf(df, n_components, fitted_vectorisers, random_state=None):\n",
    "    \"\"\"\n",
    "    Incrementally update fitted TF-IDF vectorisers and perform Latent Semantic Analysis (LSA)\n",
    "    \"\"\"\n",
    "\n",
    "    # Update the TF-IDF vectorisers with a data chunk\n",
    "    inc_fitted_vectorisers = increment_fit_vectorisers(df, fitted_vectorisers)\n",
    "\n",
    "    # Get updated TF-IDF matrices from the updated vectorisers\n",
    "    tfidf_matrices = get_tfidf_matrices(inc_fitted_vectorisers)\n",
    "\n",
    "    # Perform LSA on updated TF-IDF matrices\n",
    "    tfidf_lsas, explained_variances = lsa_selection(\n",
    "        tfidf_matrices, n_components, random_state\n",
    "    )\n",
    "\n",
    "    # Return the LSA-transformed TF-IDF matrices, their explained variances, and the updated vectorisers\n",
    "    return tfidf_lsas, explained_variances, inc_fitted_vectorisers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23ec26-1d75-4d26-b45a-99c425d58e1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model training & evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c4e9d-4e95-4d1d-86cf-6520baf77228",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c2086-25bb-4870-88cd-d10dc6b68f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def subsample(X, sample_size, seed, labels=None):\n",
    "    \"\"\"\n",
    "    Take random sample from X\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    random_indices = np.random.choice(X.shape[0], size=sample_size, replace=False)\n",
    "\n",
    "    X_subsample = X[random_indices]\n",
    "\n",
    "    labels_subsample = None\n",
    "\n",
    "    if labels is not None:\n",
    "        labels_subsample = labels[random_indices]\n",
    "\n",
    "    return (X_subsample, labels_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0f056-5a93-4d7e-8c06-02030f84e534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_BIRCH(X, branching_factor, threshold):\n",
    "    \"\"\"\n",
    "    Train BIRCH clustering algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a BIRCH clustering instance\n",
    "    birch_clustering = Birch(\n",
    "        n_clusters=None, branching_factor=branching_factor, threshold=threshold\n",
    "    )\n",
    "\n",
    "    # Fit the BIRCH clustering model on the TF-IDF data\n",
    "    birch_clustering.fit(X)\n",
    "\n",
    "    return birch_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3862d3e-66e3-4f10-8799-4aeae487d24b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0bba9-70cd-4e57-9d05-d6dda65b4d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X, labels):\n",
    "    \"\"\"\n",
    "    Evaluate resulting model\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate Silhouette Score\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "\n",
    "    # Calculate Calinski-Harabasz Index\n",
    "    ch_index = calinski_harabasz_score(X, labels)\n",
    "\n",
    "    # Calculate Davies-Bouldin Index\n",
    "    db_index = davies_bouldin_score(X, labels)\n",
    "\n",
    "    return (sil_score, ch_index, db_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6270e6-5f9d-414b-87cf-36c8f0e48896",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b487e3-43ac-42f0-80af-46475d0b9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = load_data(\"datasets/kaggle_siddharta_malicious_benign.csv\")\n",
    "urls_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488794c-45fe-45e8-843f-0902d3f2c705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_extracted_df = extract_features(urls_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f1e25-52bb-4d4a-8fa2-720035241295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_matrix = transform_features(urls_extracted_df, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5826c99-2f03-4ed1-a261-d53262400ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_lsas, explained_variances, _ = preprocess_tfidf(\n",
    "    urls_extracted_df,\n",
    "    [\"domainTLD\", \"subdomain\"],\n",
    "    [(5, 8), (2, 5)],\n",
    "    [138, 121],\n",
    "    random_state=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a031b3-be7e-477f-a170-1fbfc476766c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all_nonstandardised = np.hstack([urls_matrix] + tfidf_lsas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50c8c2-4f64-4840-a42a-9a932a85ecc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all = standardise_features(X_all_nonstandardised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a7bf1-2e1b-4b56-88de-13206890d5ff",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66cc7fe-416b-4c3f-978d-9c206ce0c888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "birch = train_BIRCH(X_all, 265, 4.07)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "labels = birch.labels_\n",
    "X_subsample, labels_subsample = subsample(X_all, 20000, 24, labels)\n",
    "metrics = evaluate_model(X_subsample, labels_subsample)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Silhouette Score: \", metrics[0])\n",
    "print(\"Calinski-Harabasz Index: \", metrics[1])\n",
    "print(\"Davies-Bouldin Index: \", metrics[2])\n",
    "\n",
    "print(f\"Num of clusters: {len(np.unique(labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1fa3c-190c-4168-b75c-8edf2d7babb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4fc6c-1db1-4a63-9e29-350e9f82e346",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca5163-6fb1-4df7-bf85-90c144df32a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVE_FIGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff935d24-4bef-4257-b050-da3bc2c38423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc = {\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"figure.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.size\": 16,\n",
    "}\n",
    "\n",
    "sns.set_theme(rc=rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957bc83d-9ed6-430b-983b-630a043a3f60",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832df983-cb86-499e-a31c-eacf1820f461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with FQDNs and their corresponding cluster\n",
    "fqdn_cluster = pd.DataFrame({\"FQDN\": urls_extracted_df[\"FQDN\"], \"cluster\": labels})\n",
    "\n",
    "# Sort the DataFrame in reproducible fashion\n",
    "fqdn_cluster = fqdn_cluster.sort_values(\"FQDN\", ignore_index=True)\n",
    "\n",
    "# Create a dictionary that maps old cluster IDs to new ones\n",
    "cluster_mapping = {\n",
    "    old_cluster: new_cluster\n",
    "    for new_cluster, old_cluster in enumerate(fqdn_cluster[\"cluster\"].unique(), start=1)\n",
    "}\n",
    "\n",
    "# Reassign cluster column with reproducible cluster assignment\n",
    "fqdn_cluster[\"cluster\"] = fqdn_cluster[\"cluster\"].map(cluster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db30ea-5cf6-4444-a46f-21989d2dfede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fqdn_cluster.to_csv(\n",
    "    \"results/kaggle_siddharta/fqdn_cluster_reproducible.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f8f1d-e9fb-4328-bcac-ee21c055286b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group the FQDNs by their clusters\n",
    "grouped_fqdns = fqdn_cluster.groupby(\"cluster\")[\"FQDN\"].apply(list)\n",
    "\n",
    "cluster_sizes = fqdn_cluster.groupby(\"cluster\").size()\n",
    "\n",
    "cluster_size_counts = cluster_sizes.value_counts().sort_index()\n",
    "\n",
    "fqdns_per_cluster = cluster_size_counts * cluster_size_counts.index\n",
    "fqdns_per_cluster.name = \"FQDNs\"\n",
    "fqdns_per_cluster.sort_index(inplace=True)\n",
    "\n",
    "fqdns_per_cluster_repeated = fqdns_per_cluster.index.repeat(fqdns_per_cluster.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69728aa6-ffd4-4f2f-806b-5a58593c22f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_sizes_copy = cluster_sizes.copy()\n",
    "cluster_sizes_copy.index = pd.Index([\"size\"] * len(cluster_sizes_copy))\n",
    "fqdns_per_cluster_copy = fqdns_per_cluster_repeated.to_series().copy()\n",
    "fqdns_per_cluster_copy.index = pd.Index([\"fqdn\"] * len(fqdns_per_cluster_copy))\n",
    "\n",
    "hue_violin_data = (\n",
    "    pd.concat((cluster_sizes_copy, fqdns_per_cluster_copy), axis=0)\n",
    "    .to_frame(\"value\")\n",
    "    .reset_index(names=[\"category_val\"])\n",
    ")\n",
    "\n",
    "del cluster_sizes_copy\n",
    "del fqdns_per_cluster_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93737d-c7cb-4dca-8c24-f39d7ef13559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"results/kaggle_siddharta/cluster_fqdns_list_reproducible.txt\", \"w\") as f:\n",
    "    f.write(\"Cluster | FQDNs\\n\")\n",
    "\n",
    "    for cluster, fqdns in grouped_fqdns.iteritems():\n",
    "        fqdns_str = \", \".join(fqdns)\n",
    "        f.write(f\"{cluster} | {fqdns_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff61567-606c-4370-807d-259ce2ea2463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Description of FQDN distribtuion among clusters\")\n",
    "fqdns_per_cluster_repeated.to_series().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cbb7b-544e-4e1a-831e-8c92a1dc6fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(np.bincount(fqdn_cluster[\"cluster\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372b16b-dd45-4155-bdeb-16a42a824141",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## t-SNE attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb08835-fa7a-4bf3-a17e-a95639dc6b3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recalculation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6503496-ffb6-413f-af5a-2276a43f51db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T10:24:47.991252Z",
     "iopub.status.busy": "2023-05-08T10:24:47.990769Z",
     "iopub.status.idle": "2023-05-08T10:42:11.297863Z",
     "shell.execute_reply": "2023-05-08T10:42:11.295325Z",
     "shell.execute_reply.started": "2023-05-08T10:24:47.991214Z"
    },
    "tags": []
   },
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_data = tsne.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e303546-0b1c-43ef-8566-f70f484691f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:08:57.001024Z",
     "iopub.status.busy": "2023-05-08T13:08:57.000375Z",
     "iopub.status.idle": "2023-05-08T13:09:01.651527Z",
     "shell.execute_reply": "2023-05-08T13:09:01.649165Z",
     "shell.execute_reply.started": "2023-05-08T13:08:57.000960Z"
    },
    "tags": []
   },
   "source": [
    "# Create a scatter plot\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap=\"viridis_r\")\n",
    "plt.title(\"BIRCH Clustering Results\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d9426de-0e5c-4922-a719-0d8ac3f789f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:06:24.196469Z",
     "iopub.status.busy": "2023-05-08T13:06:24.195859Z",
     "iopub.status.idle": "2023-05-08T13:06:24.210023Z",
     "shell.execute_reply": "2023-05-08T13:06:24.208790Z",
     "shell.execute_reply.started": "2023-05-08T13:06:24.196416Z"
    },
    "tags": []
   },
   "source": [
    "with open(\"tsne_reduced_X_all_reproducible.pkl\", \"wb\") as file:\n",
    "    pickle.dump(reduced_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd414e-05a0-45af-892b-06405181489c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Loading pre-computed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464597c1-265d-455d-a653-5922ddb556c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"visualisations/tsne_reduced_X_all_reproducible.pkl\", \"rb\") as file:\n",
    "    tsne_data = pickle.load(file)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(tsne_data[:, 0], tsne_data[:, 1], c=labels, cmap=\"viridis_r\")\n",
    "plt.title(\"BIRCH Clustering Results\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5340e9b-07b3-4129-99ad-3128fe4084de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## UMAP attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07916fca-f524-48c3-9782-65cfaa3e75ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Recalculation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5783e6c-50cb-4ee4-91b2-6df582787095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:39:06.889405Z",
     "iopub.status.busy": "2023-05-08T11:39:06.888667Z",
     "iopub.status.idle": "2023-05-08T11:50:03.010476Z",
     "shell.execute_reply": "2023-05-08T11:50:03.008814Z",
     "shell.execute_reply.started": "2023-05-08T11:39:06.889341Z"
    },
    "tags": []
   },
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "umap_t = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
    "embedding = umap_t.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13f178c8-c3c4-4090-b48a-a39c90953c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T11:39:06.889405Z",
     "iopub.status.busy": "2023-05-08T11:39:06.888667Z",
     "iopub.status.idle": "2023-05-08T11:50:03.010476Z",
     "shell.execute_reply": "2023-05-08T11:50:03.008814Z",
     "shell.execute_reply.started": "2023-05-08T11:39:06.889341Z"
    },
    "tags": []
   },
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(\n",
    "    x=embedding[:, 0],\n",
    "    y=embedding[:, 1],\n",
    "    hue=labels,\n",
    "    palette=\"Spectral\",\n",
    "    linewidth=0,\n",
    "    alpha=0.5,\n",
    "    rasterized=True,  # helps with rendering speed when there are many points\n",
    ")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.title(\"UMAP Visualization of Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a9a0f3f-e6e6-4d61-9382-876618fc177d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:07:08.131255Z",
     "iopub.status.busy": "2023-05-08T13:07:08.130651Z",
     "iopub.status.idle": "2023-05-08T13:07:08.145777Z",
     "shell.execute_reply": "2023-05-08T13:07:08.144573Z",
     "shell.execute_reply.started": "2023-05-08T13:07:08.131203Z"
    },
    "tags": []
   },
   "source": [
    "with open(\"umap_reduced_X_all_reproducible.pkl\", \"wb\") as file:\n",
    "    pickle.dump(embedding, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376eb72-8dbb-43bf-866c-9e4dd36e23a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Loading pre-computed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b41c1f-1959-4a18-862c-4e0023a33ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"visualisations/umap_reduced_X_all_reproducible.pkl\", \"rb\") as file:\n",
    "    embedding = pickle.load(file)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(\n",
    "    x=embedding[:, 0],\n",
    "    y=embedding[:, 1],\n",
    "    hue=labels,\n",
    "    palette=\"Spectral\",\n",
    "    linewidth=0,\n",
    "    alpha=0.5,\n",
    "    rasterized=True,  # helps with rendering speed when there are many points\n",
    ")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.title(\"UMAP Visualization of Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095eaa8-fe6b-4bc1-84dd-02ca041c1b1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7fb9b-fd49-4c4f-870c-4bea424d5e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of bins to use\n",
    "num_bins = 100\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 15))\n",
    "\n",
    "# Create a sequence of log-spaced bin edges\n",
    "bins_ax = np.logspace(\n",
    "    0,\n",
    "    np.log10(max(max(cluster_sizes), max(fqdns_per_cluster_repeated)) + 1),\n",
    "    num=num_bins,\n",
    ")\n",
    "\n",
    "sns.histplot(cluster_sizes, stat=\"count\", bins=bins_ax, log=True, ax=ax1)\n",
    "ax1.set_xlabel(\"Cluster Size (log)\")\n",
    "ax1.set_ylabel(\"Cluster count (log)\")\n",
    "ax1.set_title(\"Distribution of Cluster Sizes and FQDNs\")\n",
    "ax1.set_xscale(\"log\")\n",
    "\n",
    "\n",
    "sns.histplot(\n",
    "    fqdns_per_cluster_repeated,\n",
    "    stat=\"count\",\n",
    "    bins=bins_ax,\n",
    "    color=\"orange\",\n",
    "    log=True,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_xlabel(\"Cluster Size(log)\")\n",
    "ax2.set_ylabel(\"FQDN count (log)\")\n",
    "\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_ybound(ax2.get_xbound()[0])\n",
    "\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/histogram-overall-log.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/histogram-overall-log.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0c92e-d3e1-4918-a2e9-2bf061c226c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_count = 100\n",
    "bin_width = (cluster_sizes.max() - cluster_sizes.min()) / bin_count\n",
    "\n",
    "plt.figure(figsize=(9.5, 5))\n",
    "sns.histplot(cluster_sizes, bins=bin_count, stat=\"count\", log=True)\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Distribution of Cluster Sizes\")\n",
    "plt.annotate(f\"Bin width: {bin_width:.2f}\", xy=(0.70, 0.85), xycoords=\"axes fraction\")\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/histogram-overall-sizes.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/histogram-overall-sizes.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c5f23-fcf9-45c1-adfc-dffc7151897f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_width = 10\n",
    "bins = np.arange(0, 1101, bin_width)\n",
    "\n",
    "plt.hist(cluster_sizes, bins=bins, log=True)\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Distribution of Cluster Sizes\")\n",
    "plt.annotate(f\"Bin width: {bin_width}\", xy=(0.70, 0.85), xycoords=\"axes fraction\")\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/histogram-0-1100-sizes.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/histogram-0-1100-sizes.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72d73a-e31b-445f-8a5a-2ae90167f458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_width = 5\n",
    "bins = np.arange(0, 401, bin_width) - 0.5\n",
    "\n",
    "plt.figure(figsize=(9.5, 5))\n",
    "\n",
    "plt.hist(cluster_sizes, bins=bins, log=True)\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Distribution of Cluster Sizes\")\n",
    "plt.annotate(f\"Bin width: {bin_width}\", xy=(0.75, 0.85), xycoords=\"axes fraction\")\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/histogram-0-400-sizes.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/histogram-0-400-sizes.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e317f-c383-4c1c-a8d4-d9eeb3e30a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_width = 4\n",
    "bins = np.arange(0, 201, bin_width) - 0.5\n",
    "\n",
    "plt.hist(cluster_sizes, bins=bins, log=True)\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Distribution of Cluster Sizes\")\n",
    "plt.annotate(f\"Bin width: {bin_width}\", xy=(0.70, 0.85), xycoords=\"axes fraction\")\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/histogram-0-200-sizes.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/histogram-0-200-sizes.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5c443-af91-403c-8ae9-2c3a05bc4504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_width = 1\n",
    "bins = np.arange(0, 101, bin_width) - 0.5\n",
    "\n",
    "plt.hist(cluster_sizes, bins=bins, log=True)\n",
    "plt.xlabel(\"Cluster Size\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(\"Distribution of Cluster Sizes\")\n",
    "plt.annotate(f\"Bin width: {bin_width}\", xy=(0.70, 0.85), xycoords=\"axes fraction\")\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/histogram-0-100-sizes.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/histogram-0-100-sizes.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05aaffe-19f4-43c5-95ae-fc8cfa97061b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Specific cluster ngram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815c8ba-d32e-407d-89cd-fbbe2bba2d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_sizes[cluster_sizes == max(cluster_sizes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80222bd1-e4e5-4cd8-9281-7e88efc08186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_most_frequent_ngrams(fqdns, n=3, top_k=5):\n",
    "    \"\"\"\n",
    "    Extract top_k most frequent n_grams from cluster\n",
    "    \"\"\"\n",
    "\n",
    "    ngrams_counter = Counter()\n",
    "    for fqdn in fqdns:\n",
    "        for i in range(len(fqdn) - n + 1):\n",
    "            ngrams_counter[fqdn[i : i + n]] += 1\n",
    "    return ngrams_counter.most_common(top_k)\n",
    "\n",
    "\n",
    "# Analyze a specific cluster, e.g., cluster 0\n",
    "cluster_fqdns = fqdn_cluster[fqdn_cluster[\"cluster\"] == 86][\"FQDN\"]\n",
    "for n in range(1, 10):\n",
    "    most_frequent_ngrams = extract_most_frequent_ngrams(cluster_fqdns, n)\n",
    "    print(most_frequent_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec1e68-0728-4a83-b10f-dc218e1b2fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5513f-4408-43d9-b484-bd5079044899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(12, 12))\n",
    "\n",
    "sns.boxplot(x=cluster_sizes, ax=axes[0])\n",
    "axes[0].set_xlabel(\"Cluster Size\")\n",
    "axes[0].set_title(\"Box Plot of Clusters of size x\")\n",
    "axes[0].set_xscale(\"log\")\n",
    "\n",
    "sns.boxplot(x=fqdns_per_cluster_repeated, ax=axes[1])\n",
    "axes[1].set_xlabel(\"Cluster Size\")\n",
    "axes[1].set_title(\"Box Plot of FQDNS\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/box-overall-log.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/box-overall-log.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e175cd3-1a43-4ca0-835e-94e924e676f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f3c49-a220-45c6-86a4-fb32eb1ab5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(\n",
    "    x=cluster_sizes,\n",
    "    cut=0,\n",
    "    color=\"#ff7373\",\n",
    "    inner=\"points\",\n",
    ")\n",
    "plt.xlabel(\"Cluster Size\", fontsize=22)\n",
    "plt.ylabel(\"Number of Clusters\", fontsize=22)\n",
    "plt.title(\"Violin Plot of Cluster Sizes\", fontsize=26)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/violin-sizes-log.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/violin-sizes-log.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff7bfc-95aa-4232-9d9b-6d2195adad69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(\n",
    "    x=fqdns_per_cluster_repeated, cut=0, color=\"green\", inner=\"quartile\", bw=\"silverman\"\n",
    ")\n",
    "plt.xlabel(\"Cluster Size\", fontsize=22)\n",
    "plt.ylabel(\"Number of Clusters\", fontsize=22)\n",
    "plt.title(\"Violin Plot of FQDNs\", fontsize=26)\n",
    "\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/violin-fqdns-norm.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/violin-fqdns-norm.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c85a2-3485-4f72-bfc3-e886c6f32b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "ax = sns.violinplot(\n",
    "    x=fqdns_per_cluster_repeated,\n",
    "    cut=0,\n",
    "    palette=\"viridis\",\n",
    "    inner=\"quartile\",\n",
    ")\n",
    "plt.xlabel(\"Cluster Size (log scale)\", fontsize=22)\n",
    "plt.ylabel(\"Number of Clusters\", fontsize=22)\n",
    "plt.title(\"Violin Plot of FQDNs\", fontsize=26)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "# Add a logarithmic grid with light grey color\n",
    "ax.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5, color=(0.7, 0.7, 0.7))\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/violin-fqdns-log.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/violin-fqdns-log.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4f3d3-779a-4873-b292-c699b87e6778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 7))\n",
    "\n",
    "sns.stripplot(x=cluster_sizes, palette=\"viridis\", jitter=0.4, ax=ax1)\n",
    "ax1.set_xlabel(\"Cluster Size (log scale)\", fontsize=22)\n",
    "ax1.set_ylabel(\"Dot = cluster\", fontsize=22)\n",
    "ax1.set_title(\"Strip Plots of Cluster Sizes and FQDNs\", fontsize=26)\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.tick_params(labelsize=20)\n",
    "\n",
    "sns.stripplot(x=fqdns_per_cluster_repeated, palette=\"viridis\", jitter=0.4, ax=ax2)\n",
    "ax2.set_xlabel(\"Cluster Size (log scale)\", fontsize=22)\n",
    "ax2.set_ylabel(\"Dot = FQDN\", fontsize=22)\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.tick_params(labelsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/strip-overall-log.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/strip-overall-log.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122e143-1e47-4f06-95a1-627daab8bf0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.violinplot(\n",
    "    data=hue_violin_data,\n",
    "    x=\"value\",\n",
    "    y=\"category_val\",\n",
    "    cut=0,\n",
    "    scale=\"width\",\n",
    "    inner=\"quartile\",\n",
    "    palette=\"tab10\",\n",
    ")\n",
    "plt.xlabel(\"Cluster Size\", fontsize=22)\n",
    "plt.ylabel(\"Number of: \", fontsize=22)\n",
    "plt.title(\"Violin Plot of Cluster Sizes and FQDN counts\", fontsize=26)\n",
    "\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.set_yticklabels([\"Clusters of size x\", \"FQDNs\"], rotation=90, va=\"center\")\n",
    "\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/violin-overall-normal.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/violin-overall-normal.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34b312-36d0-422d-909a-4483e7ba2047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.violinplot(\n",
    "    data=hue_violin_data,\n",
    "    x=\"value\",\n",
    "    y=\"category_val\",\n",
    "    cut=0,\n",
    "    scale=\"width\",\n",
    "    palette=\"tab10\",\n",
    "    inner=\"quartile\",\n",
    ")\n",
    "plt.xlabel(\"Cluster Size (log)\", fontsize=22)\n",
    "plt.ylabel(\"Number of: \", fontsize=22)\n",
    "plt.title(\"Violin Plot of Cluster Sizes and FQDN counts (log)\", fontsize=26)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.set_yticklabels([\"Clusters of size x\", \"FQDNs\"], rotation=90, va=\"center\")\n",
    "\n",
    "ax.grid(True, which=\"major\", linestyle=\"--\", alpha=0.5, color=(0.7, 0.7, 0.7))\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/violin-overall-log.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/violin-overall-log.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f7be9-5555-4687-ac7b-a92bd10d6880",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bubble plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7178f-1da2-424f-9e54-908c79601cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bubble plot\n",
    "cluster_size_counts = fqdns_per_cluster.sort_index()\n",
    "bubble_sizes = cluster_size_counts.values  # Adjust the scaling factor as needed\n",
    "\n",
    "# Generate random y-coordinates within a small range\n",
    "np.random.seed(42)\n",
    "random_y_coords = np.random.uniform(-0.1, 0.1, len(cluster_size_counts))\n",
    "\n",
    "# Normalize x-coordinates for colormap and apply log transformation\n",
    "log_min = np.log10(cluster_size_counts.index.min())\n",
    "log_max = np.log10(cluster_size_counts.index.max())\n",
    "log_x_coords = (np.log10(cluster_size_counts.index) - log_min) / (log_max - log_min)\n",
    "\n",
    "\n",
    "colormap = cm.get_cmap(\"tab20b\")\n",
    "colors = colormap(log_x_coords)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.scatter(\n",
    "    cluster_size_counts.index,\n",
    "    random_y_coords,\n",
    "    s=bubble_sizes,\n",
    "    alpha=0.5,\n",
    "    c=colors,\n",
    ")\n",
    "plt.xlabel(\"Cluster Size (Log Scale)\", fontsize=18)\n",
    "plt.title(\"Number of FQDNs in Clusters of Specific Size (Log Scale)\", fontsize=20)\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5, color=(0.7, 0.7, 0.7))\n",
    "\n",
    "plt.annotate(\n",
    "    \"Bubble size: number of FQDNs\",\n",
    "    xy=(0.5, -0.11),\n",
    "    xycoords=\"axes fraction\",\n",
    "    fontsize=18,\n",
    "    ha=\"center\",\n",
    "    va=\"top\",\n",
    ")\n",
    "\n",
    "if SAVE_FIGS:\n",
    "\n",
    "    plt.savefig(\n",
    "        \"visualisations/svg/bubble-overall-log.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"visualisations/pdf/bubble-overall-log.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "        dpi=600,\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b32e8-ce3c-413e-9fb2-60f1f56ea494",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Time testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9c33b-0fc3-44f7-a277-71fc3129c3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reload_df_time():\n",
    "    \"\"\"\n",
    "    Reload dataset used for time and memory measurements\n",
    "    \"\"\"\n",
    "    df_time1 = load_data(\"datasets/kaggle_siddharta_malicious_benign.csv\")\n",
    "    df_time2 = load_data(\"datasets/openphis_phishingURLs.csv\")\n",
    "    df_time3 = load_data(\"datasets/advisor-sent-data.csv\")\n",
    "\n",
    "    df_time = pd.concat((df_time1, df_time2, df_time3), axis=0)\n",
    "\n",
    "    del df_time1\n",
    "    del df_time2\n",
    "    del df_time3\n",
    "\n",
    "    df_time = clean_data(extract_FQDNs(df_time))\n",
    "\n",
    "    return df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb91ab-1d0c-4587-b98b-937ecfe68c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_test_example(df_time_sample):\n",
    "    \"\"\"\n",
    "    Run single time and memory test\n",
    "    \"\"\"\n",
    "\n",
    "    df_time_extracted = extract_features(df_time_sample)\n",
    "\n",
    "    df_time_matrix = transform_features(df_time_extracted, random_state=12)\n",
    "\n",
    "    tfidf_lsas_time, _, _ = preprocess_tfidf(\n",
    "        df_time_extracted,\n",
    "        [\"domainTLD\", \"subdomain\"],\n",
    "        [(5, 8), (2, 5)],\n",
    "        [138, 121],\n",
    "        random_state=123,\n",
    "    )\n",
    "\n",
    "    X_all_nonstandardised_time = np.hstack([df_time_matrix] + tfidf_lsas_time)\n",
    "\n",
    "    X_all_time = standardise_features(X_all_nonstandardised_time)\n",
    "\n",
    "    birch = train_BIRCH(X_all_time, 265, 4.07)\n",
    "\n",
    "    _ = birch.labels_\n",
    "\n",
    "    process = psutil.Process()\n",
    "    memory_usage = process.memory_info().rss\n",
    "\n",
    "    print(f\"Run with df of shape {df_time_sample.shape}; used {memory_usage} memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf91f5b-789c-42f8-8cf8-313d6d081616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_process(process):\n",
    "    \"\"\"\n",
    "    Start and join a process\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the process\n",
    "    process.start()\n",
    "\n",
    "    # Wait for the process to finish\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ee8b1-d583-4db7-be43-436e7edcfc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sizes = [1, 10, 100, 1000, 10000, 100000, 200000]\n",
    "\n",
    "# Measure time and memory resources of runs over multiple sample sizes\n",
    "for n in sample_sizes:\n",
    "\n",
    "    df_time = reload_df_time()\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    random_indices = np.random.choice(df_time.shape[0], size=n, replace=False)\n",
    "\n",
    "    df_time_sample = df_time.iloc[random_indices, :]\n",
    "\n",
    "    del df_time\n",
    "\n",
    "    # Create a new process for each run\n",
    "    process = mp.Process(target=run_test_example, args=(df_time_sample,))\n",
    "\n",
    "    %time start_process(process)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c557637-b0cf-4424-95d2-b67cc0c82c40",
   "metadata": {},
   "source": [
    "(1, 1)\n",
    "Run with df of shape (1, 2); used 280227840 memory\n",
    "CPU times: user 7.82 ms, sys: 16.5 ms, total: 24.3 ms\n",
    "Wall time: 32.8 s\n",
    "(10, 5)\n",
    "Run with df of shape (10, 2); used 311386112 memory\n",
    "CPU times: user 5.52 ms, sys: 15 ms, total: 20.5 ms\n",
    "Wall time: 35.5 s\n",
    "(100, 10)\n",
    "Run with df of shape (100, 2); used 388083712 memory\n",
    "CPU times: user 8.07 ms, sys: 18.8 ms, total: 26.8 ms\n",
    "Wall time: 2min 46s\n",
    "(998, 10)\n",
    "Run with df of shape (1000, 2); used 548667392 memory\n",
    "CPU times: user 11.9 ms, sys: 20.3 ms, total: 32.2 ms\n",
    "Wall time: 4min 24s\n",
    "(9998, 12)\n",
    "Run with df of shape (10000, 2); used 678363136 memory\n",
    "CPU times: user 14.6 ms, sys: 23.6 ms, total: 38.2 ms\n",
    "Wall time: 5min 5s\n",
    "(99954, 12)\n",
    "Run with df of shape (100000, 2); used 1860886528 memory\n",
    "CPU times: user 18.1 ms, sys: 21.5 ms, total: 39.6 ms\n",
    "Wall time: 7min 39s\n",
    "(199902, 12)\n",
    "Run with df of shape (200000, 2); used 3130380288 memory\n",
    "CPU times: user 29.3 ms, sys: 21.1 ms, total: 50.4 ms\n",
    "Wall time: 10min 54s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f1df6-0370-421f-b356-9ff9fc952bcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Incremental model training example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ade15-659a-4d1f-8fc5-fbd04442d8e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## First increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bed4c-8a73-4179-ab38-6c2e764737f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "random_indices = np.random.choice(urls_df.shape[0], size=20000, replace=False)\n",
    "\n",
    "urls_df_example = urls_df.iloc[random_indices, :]\n",
    "urls_df_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2111eec-65d0-44cf-b73a-057d63470c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_extracted_example = extract_features(urls_df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90fb37-69cf-425c-8b52-8756067507c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_extracted_example.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66bb80-14cf-4895-abcc-15b9df7529e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_matrix_example = transform_features(urls_extracted_example, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3a3f3-9b97-45a9-99a4-cb60693ff1c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_lsas_example, explained_variances_example, vectorisers_example = preprocess_tfidf(\n",
    "    urls_extracted_example,\n",
    "    [\"domainTLD\", \"subdomain\"],\n",
    "    [(5, 8), (2, 5)],\n",
    "    [138, 121],\n",
    "    random_state=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0d314-b057-4343-af43-4f3172cb33dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all_nonstandardised_example = np.hstack([urls_matrix_example] + tfidf_lsas_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01834b38-99d0-4796-8e2d-327c718c507e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all_example = standardise_features(X_all_nonstandardised_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a9b72-63bf-4d13-b39b-eb9b8e9f0460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17977e9f-036d-402a-bb82-8e2af6a8b08f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "birch = train_BIRCH(X_all_example, 265, 4.07)\n",
    "# Get the cluster labels for each data point\n",
    "labels = birch.labels_\n",
    "\n",
    "metrics = evaluate_model(X_all_example, labels)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Silhouette Score: \", metrics[0])\n",
    "print(\"Calinski-Harabasz Index: \", metrics[1])\n",
    "print(\"Davies-Bouldin Index: \", metrics[2])\n",
    "\n",
    "print(f\"Num of clusters: {len([node for node in birch.subcluster_centers_])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a7d7c-5119-437b-b83d-db6892c949b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Second increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8711424-fecc-4ca5-b7e6-21239196cb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(44)\n",
    "\n",
    "random_indices = np.random.choice(urls_df.shape[0], size=20000, replace=False)\n",
    "\n",
    "urls_df_example2 = urls_df.iloc[random_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f70c6-4304-47c8-8044-6b5b59bb7efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_extracted_example2 = extract_features(urls_df_example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07a92d-3d63-407f-bc21-782b6669bff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_extracted_example2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80850608-20b5-40a4-82f8-3d4821e6f074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_matrix_example2 = transform_features(urls_extracted_example2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ed95c-d0e5-4591-957a-624ee38a4ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    tfidf_lsas_example2,\n",
    "    explained_variances_example2,\n",
    "    vectorisers_example2,\n",
    ") = increment_preprocess_tfidf(\n",
    "    df=urls_extracted_example2,\n",
    "    n_components=[138, 121],\n",
    "    fitted_vectorisers=vectorisers_example,\n",
    "    random_state=31,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c3663-771c-49b7-851b-0895c9c91ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all_nonstandardised_example2 = np.hstack(\n",
    "    [np.vstack((urls_matrix_example, urls_matrix_example2))] + tfidf_lsas_example2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6979fc-d14b-4a76-a73f-b6bdedec6cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all_example2 = standardise_features(X_all_nonstandardised_example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7194c-b4e5-442d-b2b7-49264b324760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_all_example2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a76695-d951-4eeb-a333-c9e9c2df7999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "birch = train_BIRCH(X_all_example2, 265, 4.07)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "labels_example2 = birch.labels_\n",
    "\n",
    "metrics = evaluate_model(X_all_example2, labels_example2)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Silhouette Score: \", metrics[0])\n",
    "print(\"Calinski-Harabasz Index: \", metrics[1])\n",
    "print(\"Davies-Bouldin Index: \", metrics[2])\n",
    "\n",
    "print(f\"Num of clusters: {len([node for node in birch.subcluster_centers_])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50394f-ec2b-4ece-ba88-8310d2cfe383",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e70fe-23c6-4615-8ba8-5512471c4fb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Handwritten LSA selection parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c79e3-3daf-404d-a8a9-51c5b5052f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file in binary mode\n",
    "with open(\"visualisations/domain_n_components_search.pkl\", \"rb\") as file:\n",
    "    components_variance_time_d = pickle.load(file)\n",
    "\n",
    "with open(\"visualisations/subdomain_n_components_search.pkl\", \"rb\") as file:\n",
    "    components_variance_time_s = pickle.load(file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a0147ef-e3fe-456f-9134-ab4a85351ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T12:33:53.552384Z",
     "iopub.status.busy": "2023-04-29T12:33:53.552116Z",
     "iopub.status.idle": "2023-04-29T12:33:53.559472Z",
     "shell.execute_reply": "2023-04-29T12:33:53.558642Z",
     "shell.execute_reply.started": "2023-04-29T12:33:53.552351Z"
    },
    "tags": []
   },
   "source": [
    "# Open the file in binary mode\n",
    "with open(\"visualisastions/explained_variance_ratios.pkl\", \"rb\") as file:\n",
    "\n",
    "    # Call load method to deserialze\n",
    "    explained_variance_ratios = pickle.load(file)\n",
    "\n",
    "    print(explained_variance_ratios)\n",
    "\n",
    "with open(\"visualisations/times.pkl\", \"rb\") as file:\n",
    "\n",
    "    # Call load method to deserialze\n",
    "    times = pickle.load(file)\n",
    "\n",
    "    print(times)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7690840-70ea-46e5-ba46-fd3805614429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T12:33:53.560915Z",
     "iopub.status.busy": "2023-04-29T12:33:53.560658Z",
     "iopub.status.idle": "2023-04-29T12:33:53.565410Z",
     "shell.execute_reply": "2023-04-29T12:33:53.564608Z",
     "shell.execute_reply.started": "2023-04-29T12:33:53.560880Z"
    },
    "tags": []
   },
   "source": [
    "dt = np.dtype(\"int,float\")\n",
    "\n",
    "n_components = np.asarray(times, dtype=\"i,f\")[\"f0\"]\n",
    "\n",
    "times_explained_variance = np.dstack((n_components, explained_variance_ratios))[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d006e4e0-a5cf-48d4-b8b9-f6740e3f31d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T13:11:11.967051Z",
     "iopub.status.busy": "2023-04-29T13:11:11.966144Z",
     "iopub.status.idle": "2023-04-29T13:11:11.974383Z",
     "shell.execute_reply": "2023-04-29T13:11:11.972776Z",
     "shell.execute_reply.started": "2023-04-29T13:11:11.966982Z"
    },
    "tags": []
   },
   "source": [
    "# Run to remove all collected data from TruncatedSVD analysis\n",
    "del components_variance_time_d\n",
    "del components_variance_time_s"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9a65de2-22bd-42f3-a7c4-5676b753cf7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T14:32:13.140431Z",
     "iopub.status.busy": "2023-04-29T14:32:13.139694Z",
     "iopub.status.idle": "2023-04-29T15:08:28.654859Z",
     "shell.execute_reply": "2023-04-29T15:08:28.652422Z",
     "shell.execute_reply.started": "2023-04-29T14:32:13.140363Z"
    },
    "tags": []
   },
   "source": [
    "# Calculate the explained variance ratio for different numbers of components\n",
    "# Take subsample\n",
    "SAMPLE_SIZE = int(X_domain_tfidf.shape[0] / 100)\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "random_indices = np.random.choice(\n",
    "    X_domain_tfidf.shape[0], size=SAMPLE_SIZE, replace=False\n",
    ")\n",
    "\n",
    "X_domain_tfidf_sample = X_domain_tfidf[random_indices, :]\n",
    "\n",
    "# Define search space\n",
    "start_n_components_domain = 50\n",
    "max_n_components_domain = 500\n",
    "step_domain = 50\n",
    "\n",
    "# Create DataFrame for results if it does not exist\n",
    "try:\n",
    "    components_variance_time_d\n",
    "except NameError:\n",
    "    components_variance_time_d = pd.DataFrame(\n",
    "        columns=[\"n_components\", \"explained_variance\", \"time\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Train TruncatedSVD and store results\n",
    "for n_components in range(\n",
    "    start_n_components_domain, max_n_components_domain + 1, step_domain\n",
    "):\n",
    "    start = time.time()\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=n_components), Normalizer(copy=False))\n",
    "    lsa.fit_transform(X_domain_tfidf_sample)\n",
    "    explained_variance_ratio = lsa[0].explained_variance_ratio_.sum()\n",
    "    end = time.time()\n",
    "    components_variance_time_d.loc[len(components_variance_time_d)] = (\n",
    "        n_components,\n",
    "        explained_variance_ratio,\n",
    "        end - start,\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c53e5f77-c58c-4f9e-85cc-60cbd3d46bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T15:22:14.453085Z",
     "iopub.status.busy": "2023-04-29T15:22:14.452639Z",
     "iopub.status.idle": "2023-04-29T15:22:14.472477Z",
     "shell.execute_reply": "2023-04-29T15:22:14.471024Z",
     "shell.execute_reply.started": "2023-04-29T15:22:14.453020Z"
    },
    "tags": []
   },
   "source": [
    "components_variance_time_d"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcf673cf-99a5-4b70-a54b-702fc8867486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T15:30:14.346816Z",
     "iopub.status.busy": "2023-04-29T15:30:14.346064Z",
     "iopub.status.idle": "2023-04-29T16:00:52.901475Z",
     "shell.execute_reply": "2023-04-29T16:00:52.898117Z",
     "shell.execute_reply.started": "2023-04-29T15:30:14.346754Z"
    },
    "tags": []
   },
   "source": [
    "# Calculate the explained variance ratio for different numbers of components\n",
    "# Take subsample\n",
    "SAMPLE_SIZE = int(X_subdomain_tfidf.shape[0] / 10)\n",
    "\n",
    "np.random.seed(321)\n",
    "\n",
    "random_indices = np.random.choice(\n",
    "    X_subdomain_tfidf.shape[0], size=SAMPLE_SIZE, replace=False\n",
    ")\n",
    "\n",
    "X_subdomain_tfidf_sample = X_subdomain_tfidf[random_indices, :]\n",
    "\n",
    "start_n_components_subdomain = 300\n",
    "max_n_components_subdomain = 500\n",
    "step_subdomain = 50\n",
    "\n",
    "try:\n",
    "    components_variance_time_s\n",
    "except NameError:\n",
    "    components_variance_time_s = pd.DataFrame(\n",
    "        columns=[\"n_components\", \"explained_variance\", \"time\"]\n",
    "    )\n",
    "\n",
    "for n_components in range(\n",
    "    start_n_components_subdomain, max_n_components_subdomain + 1, step_subdomain\n",
    "):\n",
    "    start = time.time()\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=n_components), Normalizer(copy=False))\n",
    "    lsa.fit_transform(X_subdomain_tfidf_sample)\n",
    "    explained_variance_ratio = lsa[0].explained_variance_ratio_.sum()\n",
    "    end = time.time()\n",
    "    components_variance_time_s.loc[len(components_variance_time_s)] = (\n",
    "        n_components,\n",
    "        explained_variance_ratio,\n",
    "        end - start,\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9c50e49-550f-4bd6-a009-6b02f8534f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T16:00:59.301841Z",
     "iopub.status.busy": "2023-04-29T16:00:59.301098Z",
     "iopub.status.idle": "2023-04-29T16:00:59.326131Z",
     "shell.execute_reply": "2023-04-29T16:00:59.324632Z",
     "shell.execute_reply.started": "2023-04-29T16:00:59.301778Z"
    },
    "tags": []
   },
   "source": [
    "components_variance_time_d = components_variance_time_d.sort_values(\n",
    "    \"n_components\"\n",
    ").reset_index(drop=True)\n",
    "components_variance_time_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a7a21-c64d-44b6-8b94-cc44cd9b9621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the cumulative explained variance ratio as a function of the number of components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    components_variance_time_d[\"n_components\"],\n",
    "    components_variance_time_d[\"explained_variance\"],\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "plt.title(\"Explained Variance Ratio vs Number of Components - Domains\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fa4e6-5a8b-4ff3-b57f-65979b8839a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the cumulative explained variance ratio as a function of the number of components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    components_variance_time_s[\"n_components\"],\n",
    "    components_variance_time_s[\"explained_variance\"],\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "plt.title(\"Explained Variance Ratio vs Number of Components - Subdomains\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ecd5e-47d9-4461-9837-e6ca93aee1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file in binary mode\n",
    "with open(\"visualisations/domain_n_components_search.pkl\", \"wb\") as file:\n",
    "    pickle.dump(components_variance_time_d, file)\n",
    "\n",
    "with open(\"visualisations/subdomain_n_components_search.pkl\", \"wb\") as file:\n",
    "    pickle.dump(components_variance_time_s, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09108549-765f-4f8a-89cc-1340987a5be3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Optuna tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a1d4d-4fbe-48c2-b100-f5c2cfa6acdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # SETUP\n",
    "\n",
    "    # BIRCH hyperparameters\n",
    "    branching_factor = trial.suggest_int(\"branching_factor\", 5, 300)\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.05, 5)\n",
    "\n",
    "    # Column selection\n",
    "    ## Trial suggest\n",
    "    columns_selected = trial.suggest_categorical(\n",
    "        \"columns_selected\",\n",
    "        [\"domainsubdomain\", \"domainsubdomainTLD\", \"domainTLDtogether_subdomain\"],\n",
    "    )\n",
    "\n",
    "    ## Options for selection\n",
    "    columns_options = [\n",
    "        [\"domain\", \"subdomain\"],\n",
    "        [\"domain\", \"subdomain\", \"TLD\"],\n",
    "        [\"domainTLD\", \"subdomain\"],\n",
    "    ]\n",
    "\n",
    "    ## Columns list creation\n",
    "    if columns_selected == \"domainsubdomain\":\n",
    "        columns = columns_options[0]\n",
    "    elif columns_selected == \"domainsubdomainTLD\":\n",
    "        columns = columns_options[1]\n",
    "    elif columns_selected == \"domainTLDtogether_subdomain\":\n",
    "        columns = columns_options[2]\n",
    "\n",
    "    # Ngram ranges for TF-IDF\n",
    "    ## Trial suggests\n",
    "    ngram_domain_or_domainTLD_low = trial.suggest_int(\n",
    "        \"ngram_domain_or_domainTLD_low\", 1, 5\n",
    "    )\n",
    "    ngram_domain_or_domainTLD_high = trial.suggest_int(\n",
    "        \"ngram_domain_or_domainTLD_high\", ngram_domain_or_domainTLD_low, 10\n",
    "    )\n",
    "\n",
    "    ngram_subdomain_low = trial.suggest_int(\"ngram_subdomain_low\", 1, 5)\n",
    "    ngram_subdomain_high = trial.suggest_int(\n",
    "        \"ngram_subdomain_high\", ngram_subdomain_low, 10\n",
    "    )\n",
    "\n",
    "    ngram_tld_low = trial.suggest_int(\"ngram_tld_low\", 1, 5)\n",
    "    ngram_tld_high = trial.suggest_int(\"ngram_tld_high\", ngram_tld_low, 10)\n",
    "\n",
    "    ## Ngram range creation\n",
    "    ngram_domain_or_domainTLD_range = (\n",
    "        ngram_domain_or_domainTLD_low,\n",
    "        ngram_domain_or_domainTLD_high,\n",
    "    )\n",
    "    ngram_subdomain_range = (ngram_subdomain_low, ngram_subdomain_high)\n",
    "    ngram_tld_range = (ngram_tld_low, ngram_tld_high)\n",
    "\n",
    "    ## Ngrams list\n",
    "    ngram_ranges = [ngram_domain_or_domainTLD_range, ngram_subdomain_range]\n",
    "\n",
    "    if len(columns) == 3:\n",
    "        ngram_ranges.append(ngram_tld_range)\n",
    "\n",
    "    # Number of components for TruncatedSVD\n",
    "    n_components_domain_or_domainTLD = trial.suggest_int(\n",
    "        \"n_components_domain_or_domainTLD\", 5, 400\n",
    "    )\n",
    "    n_components_subdomain = trial.suggest_int(\"n_components_subdomain\", 5, 400)\n",
    "    n_components_tld = trial.suggest_int(\"n_components_tld\", 5, 300)\n",
    "\n",
    "    n_components = [n_components_domain_or_domainTLD, n_components_subdomain]\n",
    "\n",
    "    if len(columns) == 3:\n",
    "        n_components.append(n_components_tld)\n",
    "\n",
    "    # EXECUTION\n",
    "    gc.disable()\n",
    "\n",
    "    start_tfidf = time.thread_time_ns()\n",
    "\n",
    "    tfidf_lsas = preprocess_tfidf(\n",
    "        columns=columns,\n",
    "        ngram_ranges=ngram_ranges,\n",
    "        n_components=n_components,\n",
    "        df=urls_extracted_df,\n",
    "    )\n",
    "\n",
    "    # For optuna testing purposes\n",
    "    # tfidf_lsas = preprocess_tfidf(\n",
    "    #     columns=[\"subdomain\"],\n",
    "    #     ngram_ranges=[(1,1)],\n",
    "    #     n_components=[5],\n",
    "    #     df=urls_extracted_df.iloc[:, 0:40000],\n",
    "    # )\n",
    "\n",
    "    X = np.hstack([urls_matrix[0:40000]] + tfidf_lsas)\n",
    "\n",
    "    end_tfidf = time.thread_time_ns()\n",
    "\n",
    "    start_birch = time.thread_time_ns()\n",
    "\n",
    "    birch = train_BIRCH(X, branching_factor=branching_factor, threshold=threshold)\n",
    "\n",
    "    end_birch = time.thread_time_ns()\n",
    "\n",
    "    gc.enable()\n",
    "\n",
    "    # Calculate time\n",
    "    birch_time = (end_birch - start_birch) / 1e9\n",
    "    tfidf_time = (end_tfidf - start_tfidf) / 1e9\n",
    "    all_time = birch_time + tfidf_time\n",
    "\n",
    "    trial.set_user_attr(\"tfidf_time\", tfidf_time)\n",
    "    trial.set_user_attr(\"birch_time\", birch_time)\n",
    "    trial.set_user_attr(\"all_time\", all_time)\n",
    "\n",
    "    labels = birch.labels_\n",
    "    X_sample, labels_sample = subsample(X, 3000, 24, labels)\n",
    "\n",
    "    num_of_clusters = len(np.bincount(labels))\n",
    "    trial.set_user_attr(\"num_of_clusters\", num_of_clusters)\n",
    "\n",
    "    metrics = evaluate_model(X_sample, labels_sample)\n",
    "\n",
    "    trial.set_user_attr(\"silhouette\", metrics[0])\n",
    "    trial.set_user_attr(\"calinski_harabasz\", metrics[1])\n",
    "    trial.set_user_attr(\"davies_bouldin\", metrics[2])\n",
    "\n",
    "    # Combine silhouette with time penalty as score\n",
    "\n",
    "    try:\n",
    "        score = metrics[0] + log(-0.2 * all_time + 1799) - 7.51\n",
    "    except ValueError:\n",
    "        score = -(2**25)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab86289a-4ad4-4316-90e7-25426a192778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T16:19:55.928360Z",
     "iopub.status.busy": "2023-05-01T16:19:55.927726Z",
     "iopub.status.idle": "2023-05-01T16:19:56.112717Z",
     "shell.execute_reply": "2023-05-01T16:19:56.111162Z",
     "shell.execute_reply.started": "2023-05-01T16:19:55.928315Z"
    },
    "tags": []
   },
   "source": [
    "optuna.delete_study(study_name=\"birch_maximisation_study\", storage=\"sqlite:///optimalisation.db\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd589a77-fce9-49b6-9197-1e29d807d65a",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%capture output\n",
    "\n",
    "study = optuna.create_study(\n",
    "    storage=\"sqlite:///optimalisation.db\",\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"birch_maximisation_study\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study.optimize(objective, n_trials=200, n_jobs=30)\n",
    "\n",
    "print(\"Trick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996292e9-1ad4-4200-915f-ec01f5a616ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "original_stdout = sys.stdout\n",
    "original_stderr = sys.stderr\n",
    "\n",
    "sys.stdout = open(\"output.log\", \"w\")\n",
    "sys.stderr = open(\"error.log\", \"w\")\n",
    "\n",
    "# Update database URL to fit your database\n",
    "with open(\".hidden/pgdb_optuna_storage_url.txt\", \"r\") as file:\n",
    "    DATABASE_URL = file.read().rstrip()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    storage=DATABASE_URL,\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"birch_maximisation_study_test\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=20, n_jobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5b6ae-e052-4344-9bf2-a478d5f639a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = original_stdout\n",
    "sys.stderr = original_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3324922-6d5c-4db3-be20-d2de7f002248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the test_study from the database\n",
    "test_study = optuna.load_study(\n",
    "    study_name=\"birch_maximisation_study\", storage=\"sqlite:///optimalisation.db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b0652-2ffe-41d3-bad7-8fddac399778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the trial results\n",
    "for trial in test_study.trials:\n",
    "    print(\n",
    "        f'Trial {trial.number}: score={trial.value}, silhouette={trial.user_attrs.get(\"silhouette\")}, time={trial.user_attrs.get(\"all_time\")}'\n",
    "    )\n",
    "\n",
    "trial = test_study.best_trial\n",
    "\n",
    "print(\n",
    "    f'Trial {trial.number}: score={trial.value}, silhouette={trial.user_attrs.get(\"silhouette\")}, time={trial.user_attrs.get(\"all_time\")}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cd121-fd55-4486-bff5-49f1778acb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for trial in test_study.trials:\n",
    "    print(trial.user_attrs.get(\"silhouette\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3e288-4e5c-467d-97db-164024ad1be6",
   "metadata": {},
   "source": [
    "### Trial exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2235787-f261-4cd5-9001-3eda83452b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optuna.visualization.matplotlib import (\n",
    "    plot_contour,\n",
    "    plot_edf,\n",
    "    plot_intermediate_values,\n",
    "    plot_optimization_history,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_param_importances,\n",
    "    plot_slice,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384528f-7e96-486c-964e-967abdc5f59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_optimization_history(test_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f91c31-3169-4982-a8ca-d3827d79e7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(test_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d427b85-a89d-4439-aa31-8911994b755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_kernel",
   "language": "python",
   "name": "bachelor_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
