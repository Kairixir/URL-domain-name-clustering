{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bcdbb2-e52e-478b-8b60-46f841aa3565",
   "metadata": {},
   "source": [
    "# Code that might be useful later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dec22e-17cb-4839-bd9f-90baf3d9204c",
   "metadata": {},
   "source": [
    "## Sklearn trick for clustering strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a20d-3098-4852-842c-f8f03e9e9392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:24:27.862539Z",
     "iopub.status.busy": "2023-04-14T18:24:27.861251Z",
     "iopub.status.idle": "2023-04-14T18:24:27.923930Z",
     "shell.execute_reply": "2023-04-14T18:24:27.895845Z",
     "shell.execute_reply.started": "2023-04-14T18:24:27.862498Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SklearnStringTrick:\n",
    "    \"\"\"\n",
    "    String clustering trick shown at:\n",
    "        https://scikit-learn.org/0.16/faq.html#how-do-i-deal-with-string-data-or-trees-graphs\n",
    "\n",
    "    Class structure allows for method reuse over distinct data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset=[]):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def lev_dist_sklearn_urls(self, x, y):\n",
    "        i, j = int(x[0]), int(y[0])  # extract indices\n",
    "        return levenshtein(self.dataset[i], self.dataset[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774571e8-f941-4ec4-8dd9-f870fe608c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:24:27.942634Z",
     "iopub.status.busy": "2023-04-14T18:24:27.939522Z",
     "iopub.status.idle": "2023-04-14T18:24:27.962753Z",
     "shell.execute_reply": "2023-04-14T18:24:27.957185Z",
     "shell.execute_reply.started": "2023-04-14T18:24:27.942554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_trick = SklearnStringTrick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f9c98-c409-401b-acc4-eda6fe3ba5ea",
   "metadata": {},
   "source": [
    "## Pattern for filtering IPv6 addresses"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bea7d64a-3c9f-47a7-a363-8046ad092d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:15:14.871410Z",
     "iopub.status.busy": "2023-04-14T18:15:14.870172Z",
     "iopub.status.idle": "2023-04-14T18:15:16.927432Z",
     "shell.execute_reply": "2023-04-14T18:15:16.926093Z",
     "shell.execute_reply.started": "2023-04-14T18:15:14.871354Z"
    },
    "tags": []
   },
   "source": [
    "# Pattern that matches all IPv6 addresses (taken from https://stackoverflow.com/a/17871737\n",
    "pattern = \"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\"\n",
    "urls_df[urls_df[\"url\"].str.match(pattern)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb629117-8e96-4ada-8734-e90b94976444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:16:40.691981Z",
     "iopub.status.busy": "2023-04-14T18:16:40.691617Z",
     "iopub.status.idle": "2023-04-14T18:16:42.286440Z",
     "shell.execute_reply": "2023-04-14T18:16:42.283345Z",
     "shell.execute_reply.started": "2023-04-14T18:16:40.691944Z"
    },
    "tags": []
   },
   "source": [
    "# To check remaining URLs\n",
    "urls_df[urls_df[\"url\"].str.match(\"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29d867-0eb1-4551-b579-83684d41dd7a",
   "metadata": {},
   "source": [
    "## My attempt for regex splitting domain and subdomains"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5bdae30-cd76-4e90-9fac-29967bb6eef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T17:05:41.041668Z",
     "iopub.status.busy": "2023-04-06T17:05:41.038512Z",
     "iopub.status.idle": "2023-04-06T17:05:41.063552Z",
     "shell.execute_reply": "2023-04-06T17:05:41.057103Z",
     "shell.execute_reply.started": "2023-04-06T17:05:41.041620Z"
    },
    "tags": []
   },
   "source": [
    "Toy code to split domain names into subdomains\n",
    "# Regex pattern to extract domain name\n",
    "pattern = r\"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?(?P<domain>[\\w\\.\\-]+)[^\\w]*\"\n",
    "\n",
    "url = pd.DataFrame([\"www.test.com\", \"www.api.test2.com\"], columns=[\"url\"])\n",
    "\n",
    "match = url[\"url\"].str.extract(pattern)\n",
    "\n",
    "pd.DataFrame(match[\"domain\"].apply(lambda x : x.split('.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293234be-c806-4062-b94d-e970b04b4d99",
   "metadata": {},
   "source": [
    "## Alternative library for tld matching"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a05c36b-03a6-4293-b581-a34ffc0ff02e",
   "metadata": {},
   "source": [
    "May be faster, too much pain for no gain\n",
    "import tld\n",
    "# Function to extract domain components using tld\n",
    "def extract_domain_components_tld(url):\n",
    "    try:\n",
    "        res = tld.get_tld(url, as_object=True, fix_protocol=True)\n",
    "        return pd.Series([res.subdomain, res.domain, res.tld, res.tld == \"\"])\n",
    "    except tld.exceptions.TldDomainNotFound:\n",
    "        return pd.Series([\"\", url, \"\", True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa4bc5-097e-4ba6-8012-6c861e554fc7",
   "metadata": {},
   "source": [
    "#### N-grams"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a4de825-bcff-4e3b-a8c9-7e8fdec2306b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T12:38:05.679608Z",
     "iopub.status.busy": "2023-04-19T12:38:05.676314Z",
     "iopub.status.idle": "2023-04-19T12:38:36.750529Z",
     "shell.execute_reply": "2023-04-19T12:38:36.749490Z",
     "shell.execute_reply.started": "2023-04-19T12:38:05.679545Z"
    },
    "tags": []
   },
   "source": [
    "# Define columns to compute Ngrams over\n",
    "NGRAM_COLUMNS = [\"domain\", \"subdomain\"]\n",
    "\n",
    "# Ngrams implementation over characters\n",
    "def ngrams(string, n):\n",
    "    return [\"\".join(string[i : i + n]) for i in range(len(string) - n + 1)]\n",
    "\n",
    "\n",
    "# Define values for n\n",
    "n_values = [2, 3]\n",
    "\n",
    "for n in n_values:\n",
    "    # Create names or new columns\n",
    "    ngram_new_columns = [f\"{col}_n{n}grams\" for col in NGRAM_COLUMNS]\n",
    "\n",
    "    # Apply vectorized function over dataframe\n",
    "    urls_df[ngram_new_columns] = urls_df[NGRAM_COLUMNS].applymap(lambda x: ngrams(x, n))\n",
    "\n",
    "urls_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f0593-23d6-4a71-99b5-ade9e056dfec",
   "metadata": {},
   "source": [
    "#### Non-functional version of TF-IDF\n",
    "\n",
    "Crashes on the .toarray() function\n",
    "Might be memory issue?\n",
    "\n",
    "TfidfVectorizer works though"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1de15ae-df69-4893-ae74-e8bff02babdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Define n-gram range\n",
    "ngram_range = (1, 3)\n",
    "\n",
    "# Compute TF-IDF features for 'domain' column\n",
    "cv_domain = CountVectorizer(ngram_range=ngram_range, analyzer=\"char\")\n",
    "\n",
    "tfidf_transformer_domain = TfidfTransformer()\n",
    "\n",
    "X_domain = cv_domain.fit_transform(urls_df[\"domain\"])\n",
    "\n",
    "X_domain_tfidf = tfidf_transformer_domain.fit_transform(X_domain)\n",
    "\n",
    "# Create a pandas DataFrame for the TF-IDF features of 'domain' column\n",
    "# tfidf_domain_df = pd.DataFrame(X_domain_tfidf.toarray(), columns=cv_domain.get_feature_names_out())\n",
    "\n",
    "# Compute TF-IDF features for 'subdomain' column\n",
    "cv_subdomain = CountVectorizer(ngram_range=ngram_range, analyzer=\"char\")\n",
    "\n",
    "tfidf_transformer_subdomain = TfidfTransformer()\n",
    "\n",
    "X_subdomain = cv_subdomain.fit_transform(urls_df[\"subdomain\"])\n",
    "\n",
    "X_subdomain_tfidf = tfidf_transformer_subdomain.fit_transform(X_subdomain)\n",
    "\n",
    "X_domain_tfidf.toarray()\n",
    "\n",
    "# Create a pandas DataFrame for the TF-IDF features of 'subdomain' column\n",
    "# tfidf_subdomain_df = pd.DataFrame(X_subdomain_tfidf.toarray(), columns=cv_subdomain.get_feature_names_out())\n",
    "\n",
    "# Combine the TF-IDF features for both columns into a single DataFrame\n",
    "# tfidf_df = pd.concat([tfidf_domain_df, tfidf_subdomain_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca4e45-fe23-4067-9545-df57a8fb5181",
   "metadata": {},
   "source": [
    "## TF-IDF using HashingVectorizer\n",
    "\n",
    "Not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97863403-a34e-4117-98ec-e17dfaae008d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-27T17:11:32.460247Z",
     "iopub.status.idle": "2023-04-27T17:11:32.461481Z",
     "shell.execute_reply": "2023-04-27T17:11:32.461146Z",
     "shell.execute_reply.started": "2023-04-27T17:11:32.461108Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "class IncrementalHashingVectorizer:\n",
    "    def __init__(self, n_features=2**18, ngram_range=(1, 1), **kwargs):\n",
    "        self.vectorizer = HashingVectorizer(\n",
    "            n_features=n_features, ngram_range=ngram_range, **kwargs\n",
    "        )\n",
    "        self.partial_fit_calls = 0\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        if self.partial_fit_calls == 0:\n",
    "            self.features_sum = np.zeros(self.vectorizer.n_features)\n",
    "            self.feature_counts = np.zeros(self.vectorizer.n_features)\n",
    "\n",
    "        transformed = self.vectorizer.transform(X)\n",
    "        self.features_sum += transformed.sum(axis=0)\n",
    "        self.feature_counts += transformed.getnnz(axis=0)\n",
    "        self.partial_fit_calls += 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed = self.vectorizer.transform(X)\n",
    "        idf_vector = np.log(self.partial_fit_calls / (1 + self.feature_counts)) + 1\n",
    "        return transformed.multiply(idf_vector)\n",
    "\n",
    "\n",
    "incremental_vectorizer = IncrementalHashingVectorizer(\n",
    "    ngram_range=NGRAM_RANGE, analyzer=\"char\"\n",
    ")\n",
    "\n",
    "\n",
    "incremental_vectorizer.partial_fit([doc])\n",
    "\n",
    "transformed_documents = incremental_vectorizer.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896d5b5-c3c7-40bf-99df-64188ef2872e",
   "metadata": {},
   "source": [
    "## TF-IDF on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a85b8-67d0-4362-b4c4-ef79eb15b595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T17:14:02.262529Z",
     "iopub.status.busy": "2023-04-27T17:14:02.259240Z",
     "iopub.status.idle": "2023-04-27T17:15:03.464503Z",
     "shell.execute_reply": "2023-04-27T17:15:03.434933Z",
     "shell.execute_reply.started": "2023-04-27T17:14:02.262459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer instance for domain column\n",
    "tfidf_domain = TfidfVectorizer(analyzer=\"char\", ngram_range=NGRAM_RANGE)\n",
    "# Fit and transform domain column\n",
    "tfidf_domain_matrix = tfidf_domain.fit_transform(urls_df[\"domain\"])\n",
    "\n",
    "# Create TfidfVectorizer instance for subdomain column\n",
    "tfidf_subdomain = TfidfVectorizer(analyzer=\"char\", ngram_range=NGRAM_RANGE)\n",
    "# Fit and transform subdomain column\n",
    "tfidf_subdomain_matrix = tfidf_subdomain.fit_transform(urls_df[\"subdomain\"])\n",
    "\n",
    "# Create pandas dataframe from tfidf_matrix\n",
    "tfidf_domain_df_tfidfv = pd.DataFrame(\n",
    "    tfidf_domain_matrix.toarray(),\n",
    "    columns=tfidf_domain.get_feature_names_out(),\n",
    "    index=urls_df.index,\n",
    ")\n",
    "tfidf_subdomain_df_tfidfv = pd.DataFrame(\n",
    "    tfidf_subdomain_matrix.toarray(),\n",
    "    columns=tfidf_subdomain.get_feature_names_out(),\n",
    "    index=urls_df.index,\n",
    ")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "# tfidf_df = pd.concat([tfidf_domain_df, tfidf_subdomain_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10256b6-5deb-4173-a5df-dca319ea3161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T17:15:03.516338Z",
     "iopub.status.busy": "2023-04-27T17:15:03.513464Z",
     "iopub.status.idle": "2023-04-27T17:15:16.918345Z",
     "shell.execute_reply": "2023-04-27T17:15:16.916741Z",
     "shell.execute_reply.started": "2023-04-27T17:15:03.516211Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 638971 entries, 0 to 638970\n",
      "Columns: 165105 entries, '01 to zzzi\n",
      "dtypes: float64(165105)\n",
      "memory usage: 786.0 GB\n"
     ]
    }
   ],
   "source": [
    "tfidf_domain_df_tfidfv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714e145-c962-4080-8519-68b0b31b3e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T17:15:16.964210Z",
     "iopub.status.busy": "2023-04-27T17:15:16.963175Z",
     "iopub.status.idle": "2023-04-27T17:15:35.073809Z",
     "shell.execute_reply": "2023-04-27T17:15:35.070903Z",
     "shell.execute_reply.started": "2023-04-27T17:15:16.964172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 638971 entries, 0 to 638970\n",
      "Columns: 162855 entries, '00 to zzzz\n",
      "dtypes: float64(162855)\n",
      "memory usage: 775.3 GB\n"
     ]
    }
   ],
   "source": [
    "tfidf_subdomain_df_tfidfv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f60d53-4bec-42a4-bdf8-d68cfb2ca033",
   "metadata": {},
   "source": [
    "## How to use LSA for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27eb6158-c22e-4540-be89-af074a5aeecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T09:00:17.552165Z",
     "iopub.status.busy": "2023-04-28T09:00:17.551430Z",
     "iopub.status.idle": "2023-04-28T09:00:32.639299Z",
     "shell.execute_reply": "2023-04-28T09:00:32.637484Z",
     "shell.execute_reply.started": "2023-04-28T09:00:17.552121Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 49.2%\n"
     ]
    }
   ],
   "source": [
    "# Taken from https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#performing-dimensionality-reduction-using-lsa\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=10), Normalizer(copy=False))\n",
    "\n",
    "X_domain_lsa = lsa.fit_transform(X_domain_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5e0b8-668a-4d7a-a383-2d91de890c51",
   "metadata": {},
   "source": [
    "### Attempts at parallelising the finding of optimal truncate ratio\n",
    "\n",
    "In the end I decided to subsample\n",
    "\n",
    "Might be useful in future development\n",
    "\n",
    "The main issue: Threads copy _as I currently believe_ the whole dataset into their processing space. Since the matrix is large it takes a bunch of memory.\n",
    "\n",
    "Possible solutions: \n",
    "- Start smaller number of workers\n",
    "- Share data among workers (read-only)\n",
    "    - Tried this, the sparse matrices cannot be shared _as I currently believe_\n",
    "    \n",
    "My solution:\n",
    "- Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3753549-c256-40a0-9c35-ebaee0b9cdee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T22:12:00.344159Z",
     "iopub.status.busy": "2023-04-28T22:12:00.343190Z",
     "iopub.status.idle": "2023-04-28T22:37:47.265777Z",
     "shell.execute_reply": "2023-04-28T22:37:47.264540Z",
     "shell.execute_reply.started": "2023-04-28T22:12:00.344095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-53:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-44:\n",
      "Process ForkPoolWorker-58:\n",
      "Process ForkPoolWorker-57:\n",
      "Process ForkPoolWorker-42:\n",
      "Process ForkPoolWorker-51:\n",
      "Process ForkPoolWorker-50:\n",
      "Process ForkPoolWorker-54:\n",
      "Process ForkPoolWorker-46:\n",
      "Process ForkPoolWorker-45:\n",
      "Process ForkPoolWorker-49:\n",
      "Process ForkPoolWorker-52:\n",
      "Process ForkPoolWorker-64:\n",
      "Process ForkPoolWorker-56:\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, shared_memory\n",
    "\n",
    "# Convert the sparse matrices to COO format\n",
    "X_domain_coo = X_domain_tfidf.tocoo()\n",
    "X_subdomain_coo = X_subdomain_tfidf.tocoo()\n",
    "\n",
    "# Create shared memory blocks for the COO data, row, and col attributes\n",
    "X_domain_data_shm = shared_memory.SharedMemory(\n",
    "    create=True, size=X_domain_coo.data.nbytes\n",
    ")\n",
    "X_domain_row_shm = shared_memory.SharedMemory(create=True, size=X_domain_coo.row.nbytes)\n",
    "X_domain_col_shm = shared_memory.SharedMemory(create=True, size=X_domain_coo.col.nbytes)\n",
    "\n",
    "X_subdomain_data_shm = shared_memory.SharedMemory(\n",
    "    create=True, size=X_subdomain_coo.data.nbytes\n",
    ")\n",
    "X_subdomain_row_shm = shared_memory.SharedMemory(\n",
    "    create=True, size=X_subdomain_coo.row.nbytes\n",
    ")\n",
    "X_subdomain_col_shm = shared_memory.SharedMemory(\n",
    "    create=True, size=X_subdomain_coo.col.nbytes\n",
    ")\n",
    "\n",
    "# Copy the COO data, row, and col attributes to shared memory\n",
    "X_domain_data_shared = np.ndarray(\n",
    "    X_domain_coo.data.shape, dtype=X_domain_coo.data.dtype, buffer=X_domain_data_shm.buf\n",
    ")\n",
    "X_domain_row_shared = np.ndarray(\n",
    "    X_domain_coo.row.shape, dtype=X_domain_coo.row.dtype, buffer=X_domain_row_shm.buf\n",
    ")\n",
    "X_domain_col_shared = np.ndarray(\n",
    "    X_domain_coo.col.shape, dtype=X_domain_coo.col.dtype, buffer=X_domain_col_shm.buf\n",
    ")\n",
    "\n",
    "X_subdomain_data_shared = np.ndarray(\n",
    "    X_subdomain_coo.data.shape,\n",
    "    dtype=X_subdomain_coo.data.dtype,\n",
    "    buffer=X_subdomain_data_shm.buf,\n",
    ")\n",
    "X_subdomain_row_shared = np.ndarray(\n",
    "    X_subdomain_coo.row.shape,\n",
    "    dtype=X_subdomain_coo.row.dtype,\n",
    "    buffer=X_subdomain_row_shm.buf,\n",
    ")\n",
    "X_subdomain_col_shared = np.ndarray(\n",
    "    X_subdomain_coo.col.shape,\n",
    "    dtype=X_subdomain_coo.col.dtype,\n",
    "    buffer=X_subdomain_col_shm.buf,\n",
    ")\n",
    "\n",
    "np.copyto(X_domain_data_shared, X_domain_coo.data)\n",
    "np.copyto(X_domain_row_shared, X_domain_coo.row)\n",
    "np.copyto(X_domain_col_shared, X_domain_coo.col)\n",
    "\n",
    "np.copyto(X_subdomain_data_shared, X_subdomain_coo.data)\n",
    "np.copyto(X_subdomain_row_shared, X_subdomain_coo.row)\n",
    "np.copyto(X_subdomain_col_shared, X_subdomain_coo.col)\n",
    "\n",
    "# Calculate the explained variance ratio for different numbers of components\n",
    "max_n_components_domain = 600  # Adjust based on your needs\n",
    "max_n_components_subdomain = 30  # Adjust based on your needs\n",
    "start_n_components_domain = 500\n",
    "start_n_components_subdomain = 10\n",
    "step_domain = 100\n",
    "step_subdomain = 10\n",
    "\n",
    "\n",
    "def lsa_variance_ratio(args):\n",
    "    (\n",
    "        data_shm_name,\n",
    "        row_shm_name,\n",
    "        col_shm_name,\n",
    "        shape,\n",
    "        dtype,\n",
    "        n_components,\n",
    "        feature,\n",
    "    ) = args\n",
    "\n",
    "    data_shm = shared_memory.SharedMemory(name=data_shm_name)\n",
    "    row_shm = shared_memory.SharedMemory(name=row_shm_name)\n",
    "    col_shm = shared_memory.SharedMemory(name=col_shm_name)\n",
    "\n",
    "    data = np.ndarray(shape[0], dtype=dtype[0], buffer=data_shm.buf)\n",
    "    row = np.ndarray(shape[1], dtype=dtype[1], buffer=row_shm.buf)\n",
    "    col = np.ndarray(shape[2], dtype=dtype[2], buffer=col_shm.buf)\n",
    "\n",
    "    # Reconstruct the sparse matrix in CSR format\n",
    "    X_tfidf_coo = sp.csr_matrix((data, (row, col)), shape=shape[3])\n",
    "    start = time.time()\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=n_components), Normalizer(copy=False))\n",
    "    lsa.fit_transform(X_tfidf_coo)\n",
    "    explained_variance_ratio = lsa[0].explained_variance_ratio_.sum()\n",
    "    end = time.time()\n",
    "\n",
    "    # Close the shared memory objects in the current process\n",
    "    data_shm.close()\n",
    "    row_shm.close()\n",
    "    col_shm.close()\n",
    "\n",
    "    return (n_components, explained_variance_ratio, end - start, feature)\n",
    "\n",
    "\n",
    "# Parallelize the for loop using multiprocessing.Pool\n",
    "with Pool() as pool:\n",
    "    tasks_domain = [\n",
    "        (\n",
    "            X_domain_data_shm.name,\n",
    "            X_domain_row_shm.name,\n",
    "            X_domain_col_shm.name,\n",
    "            (\n",
    "                X_domain_coo.nnz,\n",
    "                X_domain_coo.row.size,\n",
    "                X_domain_coo.col.size,\n",
    "                X_domain_tfidf.shape,\n",
    "            ),\n",
    "            (X_domain_coo.data.dtype, X_domain_coo.row.dtype, X_domain_coo.col.dtype),\n",
    "            n_components,\n",
    "            \"domain\",\n",
    "        )\n",
    "        for n_components in range(\n",
    "            start_n_components_domain, max_n_components_domain + 1, step_domain\n",
    "        )\n",
    "    ]\n",
    "    tasks_subdomain = [\n",
    "        (\n",
    "            X_subdomain_data_shm.name,\n",
    "            X_subdomain_row_shm.name,\n",
    "            X_subdomain_col_shm.name,\n",
    "            (\n",
    "                X_subdomain_coo.nnz,\n",
    "                X_subdomain_coo.row.size,\n",
    "                X_subdomain_coo.col.size,\n",
    "                X_subdomain_tfidf.shape,\n",
    "            ),\n",
    "            (\n",
    "                X_subdomain_coo.data.dtype,\n",
    "                X_subdomain_coo.row.dtype,\n",
    "                X_subdomain_coo.col.dtype,\n",
    "            ),\n",
    "            n_components,\n",
    "            \"subdomain\",\n",
    "        )\n",
    "        for n_components in range(\n",
    "            start_n_components_subdomain, max_n_components_subdomain + 1, step_subdomain\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    results_domain = pool.map(lsa_variance_ratio, tasks_domain)\n",
    "    results_subdomain = pool.map(lsa_variance_ratio, tasks_subdomain)\n",
    "\n",
    "times_explained_variance_domain = np.array(results_domain)\n",
    "times_explained_variance_subdomain = np.array(results_subdomain)\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open(\"domain_tev_final.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(times_explained_variance_domain, file)\n",
    "\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open(\"subdomain_tev_final.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(times_explained_variance_subdomain, file)\n",
    "\n",
    "# Clean up shared memory blocks\n",
    "X_domain_data_shm.close()\n",
    "X_domain_data_shm.unlink()\n",
    "X_domain_row_shm.close()\n",
    "X_domain_row_shm.unlink()\n",
    "X_domain_col_shm.close()\n",
    "X_domain_col_shm.unlink()\n",
    "\n",
    "X_subdomain_data_shm.close()\n",
    "X_subdomain_data_shm.unlink()\n",
    "X_subdomain_row_shm.close()\n",
    "X_subdomain_row_shm.unlink()\n",
    "X_subdomain_col_shm.close()\n",
    "X_subdomain_col_shm.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6042dd-0c95-468a-883c-d32b7accd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool, shared_memory\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Dummy data, replace with your actual data\n",
    "X_domain_tfidf = np.random.rand(10000, 2000)\n",
    "X_subdomain_tfidf = np.random.rand(10000, 2000)\n",
    "\n",
    "# Create shared memory blocks for the arrays\n",
    "X_domain_shm = shared_memory.SharedMemory(create=True, size=X_domain_tfidf.nbytes)\n",
    "X_subdomain_shm = shared_memory.SharedMemory(create=True, size=X_subdomain_tfidf.nbytes)\n",
    "\n",
    "# Copy the arrays to shared memory\n",
    "X_domain_tfidf_shared = np.ndarray(\n",
    "    X_domain_tfidf.shape, dtype=X_domain_tfidf.dtype, buffer=X_domain_shm.buf\n",
    ")\n",
    "X_subdomain_tfidf_shared = np.ndarray(\n",
    "    X_subdomain_tfidf.shape, dtype=X_subdomain_tfidf.dtype, buffer=X_subdomain_shm.buf\n",
    ")\n",
    "\n",
    "np.copyto(X_domain_tfidf_shared, X_domain_tfidf)\n",
    "np.copyto(X_subdomain_tfidf_shared, X_subdomain_tfidf)\n",
    "\n",
    "# Calculate the explained variance ratio for different numbers of components\n",
    "max_n_components = 2000  # Adjust based on your needs\n",
    "\n",
    "\n",
    "def lsa_variance_ratio(args):\n",
    "    shm_name, shape, dtype, n_components = args\n",
    "    shm = shared_memory.SharedMemory(name=shm_name)\n",
    "    X_tfidf = np.ndarray(shape, dtype=dtype, buffer=shm.buf)\n",
    "\n",
    "    start = time.time()\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=n_components), Normalizer(copy=False))\n",
    "    lsa.fit_transform(X_tfidf)\n",
    "    explained_variance_ratio = lsa[0].explained_variance_ratio_.sum()\n",
    "    end = time.time()\n",
    "\n",
    "    # Close the shared memory object in the current process\n",
    "    shm.close()\n",
    "\n",
    "    return (n_components, explained_variance_ratio, end - start)\n",
    "\n",
    "\n",
    "# Parallelize the for loop using multiprocessing.Pool\n",
    "with Pool() as pool:\n",
    "    tasks_domain = [\n",
    "        (\n",
    "            X_domain_shm.name,\n",
    "            X_domain_tfidf.shape,\n",
    "            X_domain_tfidf.dtype,\n",
    "            n_components,\n",
    "            \"domain\",\n",
    "        )\n",
    "        for n_components in range(start_n_components_domain, max_n_components_domain + 1, step_domain)\n",
    "    ]\n",
    "    tasks_subdomain = [\n",
    "        (\n",
    "            X_subdomain_shm.name,\n",
    "            X_subdomain_tfidf.shape,\n",
    "            X_subdomain_tfidf.dtype,\n",
    "            n_components,\n",
    "            \"subdomain\",\n",
    "        )\n",
    "        for n_components in range(start_n_components_subdomain, max_n_components_subdomain + 1, step_subdomain)\n",
    "    ]\n",
    "\n",
    "    results_domain = pool.map(lsa_variance_ratio, tasks_domain)\n",
    "    results_subdomain = pool.map(lsa_variance_ratio, tasks_subdomain)\n",
    "\n",
    "times_explained_variance_domain = np.array(results_domain)\n",
    "times_explained_variance_subdomain = np.array(results_subdomain)\n",
    "\n",
    "# Clean up shared memory blocks\n",
    "X_domain_shm.close()\n",
    "X_domain_shm.unlink()\n",
    "X_subdomain_shm.close()\n",
    "X_subdomain_shm.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6ed8c-7870-4fd8-ae07-203be36f547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, shared_memory\n",
    "\n",
    "# Create shared memory blocks for the arrays\n",
    "X_domain_shm = shared_memory.SharedMemory(create=True, size=X_domain_tfidf.nbytes)\n",
    "X_subdomain_shm = shared_memory.SharedMemory(create=True, size=X_subdomain_tfidf.nbytes)\n",
    "\n",
    "# Copy the arrays to shared memory\n",
    "X_domain_tfidf_shared = np.ndarray(\n",
    "    X_domain_tfidf.shape, dtype=X_domain_tfidf.dtype, buffer=X_domain_shm.buf\n",
    ")\n",
    "X_subdomain_tfidf_shared = np.ndarray(\n",
    "    X_subdomain_tfidf.shape, dtype=X_subdomain_tfidf.dtype, buffer=X_subdomain_shm.buf\n",
    ")\n",
    "\n",
    "np.copyto(X_domain_tfidf_shared, X_domain_tfidf)\n",
    "np.copyto(X_subdomain_tfidf_shared, X_subdomain_tfidf)\n",
    "\n",
    "# Calculate the explained variance ratio for different numbers of components\n",
    "max_n_components_domain = 2000  # Adjust based on your needs\n",
    "max_n_components_subdomain = 200  # Adjust based on your needs\n",
    "start_n_components_domain = 500\n",
    "start_n_components_subdomain = 10\n",
    "step_domain = 100\n",
    "step_subdomain = 10\n",
    "\n",
    "\n",
    "def lsa_variance_ratio(args):\n",
    "    X_tfidf_shm, n_components = args\n",
    "    X_tfidf = np.ndarray(\n",
    "        X_tfidf_shm.shape, dtype=X_tfidf_shm.dtype, buffer=X_tfidf_shm.buf\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=n_components), Normalizer(copy=False))\n",
    "    lsa.fit_transform(X_tfidf)\n",
    "    explained_variance_ratio = lsa[0].explained_variance_ratio_.sum()\n",
    "    end = time.time()\n",
    "    return (n_components, explained_variance_ratio, end - start)\n",
    "\n",
    "\n",
    "# Parallelize the for loop using multiprocessing.Pool\n",
    "with Pool() as pool:\n",
    "    tasks_domain = [\n",
    "        (X_domain_tfidf_shared, n_components, \"domain\")\n",
    "        for n_components in range(\n",
    "            start_n_components_domain, max_n_components_domain + 1, step_domain\n",
    "        )\n",
    "    ]\n",
    "    tasks_subdomain = [\n",
    "        (X_subdomain_tfidf_shared, n_components, \"subdomain\")\n",
    "        for n_components in range(\n",
    "            start_n_components_subdomain, max_n_components_subdomain + 1, step_subdomain\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    results_domain = pool.map(lsa_variance_ratio, tasks_domain)\n",
    "    results_subdomain = pool.map(lsa_variance_ratio, tasks_subdomain)\n",
    "\n",
    "times_explained_variance_domain = np.array(results_domain)\n",
    "times_explained_variance_subdomain = np.array(results_subdomain)\n",
    "\n",
    "# Clean up shared memory blocks\n",
    "X_domain_shm.close()\n",
    "X_domain_shm.unlink()\n",
    "X_subdomain_shm.close()\n",
    "X_subdomain_shm.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28121bc4-ba49-40fa-8452-25311de4c1a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T21:35:00.774694Z",
     "iopub.status.busy": "2023-04-28T21:35:00.773605Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Calculate the explained variance ratio for different numbers of components\n",
    "max_n_components_domain = 2000  # Adjust based on your needs\n",
    "max_n_components_subdomain = 200  # Adjust based on your needs\n",
    "start_n_components_domain = 500\n",
    "start_n_components_subdomain = 10\n",
    "step_domain = 100\n",
    "step_subdomain = 10\n",
    "\n",
    "\n",
    "def lsa_variance_ratio(X_tfidf, n_components, feature):\n",
    "    start = time.time()\n",
    "    lsa = make_pipeline(TruncatedSVD(n_components=n_components), Normalizer(copy=False))\n",
    "    lsa.fit_transform(X_tfidf)\n",
    "    explained_variance_ratio = lsa[0].explained_variance_ratio_.sum()\n",
    "    end = time.time()\n",
    "    return (n_components, explained_variance_ratio, end - start, feature)\n",
    "\n",
    "\n",
    "# Parallelize the for loop using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures_domain = [\n",
    "        executor.submit(lsa_variance_ratio, X_domain_tfidf, n_components, \"domain\")\n",
    "        for n_components in range(\n",
    "            start_n_components_domain, max_n_components_domain + 1, step_domain\n",
    "        )\n",
    "    ]\n",
    "    futures_subdomain = [\n",
    "        executor.submit(\n",
    "            lsa_variance_ratio, X_subdomain_tfidf, n_components, \"subdomain\"\n",
    "        )\n",
    "        for n_components in range(\n",
    "            start_n_components_subdomain, max_n_components_subdomain + 1, step_subdomain\n",
    "        )\n",
    "    ]\n",
    "\n",
    "times_explained_variance_domain = np.array(\n",
    "    [future.result() for future in futures_domain]\n",
    ")\n",
    "times_explained_variance_subdomain = np.array(\n",
    "    [future.result() for future in futures_subdomain]\n",
    ")\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open(\"domain_tev.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(times_explained_variance_domain, file)\n",
    "\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open(\"subdomain_tev.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(times_explained_variance_subdomain, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85b550-89d7-46d4-a077-bab53679c434",
   "metadata": {},
   "source": [
    "### Encode labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9d26654-23e1-4f66-9a74-54f558f5b58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T20:22:21.118210Z",
     "iopub.status.busy": "2023-04-29T20:22:21.117843Z",
     "iopub.status.idle": "2023-04-29T20:22:21.456427Z",
     "shell.execute_reply": "2023-04-29T20:22:21.455268Z",
     "shell.execute_reply.started": "2023-04-29T20:22:21.118180Z"
    },
    "tags": []
   },
   "source": [
    "# Create OneHotEncoded features from type\n",
    "\n",
    "\n",
    "ohenc = OneHotEncoder(sparse_output=False)\n",
    "type_ohenc = pd.DataFrame(\n",
    "    ohenc.fit_transform(urls_df[\"type\"].values.reshape(-1, 1)),\n",
    "    columns=ohenc.categories_[0],\n",
    ").astype(bool)\n",
    "\n",
    "# URLs_transformed df\n",
    "urls_tdf = pd.concat([urls_df, type_ohenc], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a101e384-7645-42b0-a836-a1f1ef9ff9f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T20:22:21.458477Z",
     "iopub.status.busy": "2023-04-29T20:22:21.457813Z",
     "iopub.status.idle": "2023-04-29T20:22:21.484217Z",
     "shell.execute_reply": "2023-04-29T20:22:21.483516Z",
     "shell.execute_reply.started": "2023-04-29T20:22:21.458447Z"
    },
    "tags": []
   },
   "source": [
    "urls_tdf[\"malicious\"] = ~urls_tdf[\"benign\"]\n",
    "urls_tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d901b-3b25-4be8-b78a-d71b146fe6ef",
   "metadata": {},
   "source": [
    "## Computing density of matrices & print variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c6f743a-a15f-4143-8cb2-c42d158c1a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T09:15:11.406418Z",
     "iopub.status.busy": "2023-05-04T09:15:11.405795Z",
     "iopub.status.idle": "2023-05-04T09:15:11.415479Z",
     "shell.execute_reply": "2023-05-04T09:15:11.414083Z",
     "shell.execute_reply.started": "2023-05-04T09:15:11.406362Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_tfidf(df, columns, ngram_range, n_components):\n",
    "    fitted_vectorizers = fit_vectorizers(df, columns, ngram_range)\n",
    "    tfidf_matrices = get_tfidf_matrices(fitted_vectorizers)\n",
    "    \n",
    "    # Compute density of tfidf output\n",
    "    for X_tfidf in tfidf_matrices:\n",
    "        print(f\"{X_tfidf.nnz / np.prod(X_tfidf.shape)}\")\n",
    "\n",
    "    # tfidf_matrices are sparse\n",
    "\n",
    "    tfidf_lsas, explained_variances = lsa_selection(tfidf_matrices, n_components)\n",
    "\n",
    "    # Compute density of LSA output\n",
    "    for tfidf_lsa in tfidf_lsas:\n",
    "        density = np.count_nonzero(tfidf_lsa) / float(tfidf_lsa.size)\n",
    "        print(density)\n",
    "\n",
    "    # tfidf_lsa are dense matrices\n",
    "\n",
    "    for i, variance in enumerate(explained_variances):\n",
    "        print(\n",
    "            f\"Explained variance of {columns[i]} in the SVD step: {variance * 100:.1f}%\"\n",
    "        )\n",
    "\n",
    "    return tfidf_lsas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelor thesis kernel",
   "language": "python",
   "name": "avast_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
