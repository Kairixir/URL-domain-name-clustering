{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bcdbb2-e52e-478b-8b60-46f841aa3565",
   "metadata": {},
   "source": [
    "# Code that might be useful later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dec22e-17cb-4839-bd9f-90baf3d9204c",
   "metadata": {},
   "source": [
    "## Sklearn trick for clustering strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a20d-3098-4852-842c-f8f03e9e9392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:24:27.862539Z",
     "iopub.status.busy": "2023-04-14T18:24:27.861251Z",
     "iopub.status.idle": "2023-04-14T18:24:27.923930Z",
     "shell.execute_reply": "2023-04-14T18:24:27.895845Z",
     "shell.execute_reply.started": "2023-04-14T18:24:27.862498Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SklearnStringTrick:\n",
    "    \"\"\"\n",
    "    String clustering trick shown at:\n",
    "        https://scikit-learn.org/0.16/faq.html#how-do-i-deal-with-string-data-or-trees-graphs\n",
    "\n",
    "    Class structure allows for method reuse over distinct data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset=[]):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def lev_dist_sklearn_urls(self, x, y):\n",
    "        i, j = int(x[0]), int(y[0])  # extract indices\n",
    "        return levenshtein(self.dataset[i], self.dataset[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774571e8-f941-4ec4-8dd9-f870fe608c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:24:27.942634Z",
     "iopub.status.busy": "2023-04-14T18:24:27.939522Z",
     "iopub.status.idle": "2023-04-14T18:24:27.962753Z",
     "shell.execute_reply": "2023-04-14T18:24:27.957185Z",
     "shell.execute_reply.started": "2023-04-14T18:24:27.942554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_trick = SklearnStringTrick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f9c98-c409-401b-acc4-eda6fe3ba5ea",
   "metadata": {},
   "source": [
    "## Pattern for filtering IPv6 addresses"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bea7d64a-3c9f-47a7-a363-8046ad092d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:15:14.871410Z",
     "iopub.status.busy": "2023-04-14T18:15:14.870172Z",
     "iopub.status.idle": "2023-04-14T18:15:16.927432Z",
     "shell.execute_reply": "2023-04-14T18:15:16.926093Z",
     "shell.execute_reply.started": "2023-04-14T18:15:14.871354Z"
    },
    "tags": []
   },
   "source": [
    "# Pattern that matches all IPv6 addresses (taken from https://stackoverflow.com/a/17871737\n",
    "pattern = \"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\"\n",
    "urls_df[urls_df[\"url\"].str.match(pattern)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb629117-8e96-4ada-8734-e90b94976444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:16:40.691981Z",
     "iopub.status.busy": "2023-04-14T18:16:40.691617Z",
     "iopub.status.idle": "2023-04-14T18:16:42.286440Z",
     "shell.execute_reply": "2023-04-14T18:16:42.283345Z",
     "shell.execute_reply.started": "2023-04-14T18:16:40.691944Z"
    },
    "tags": []
   },
   "source": [
    "# To check remaining URLs\n",
    "urls_df[urls_df[\"url\"].str.match(\"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29d867-0eb1-4551-b579-83684d41dd7a",
   "metadata": {},
   "source": [
    "## My attempt for regex splitting domain and subdomains"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5bdae30-cd76-4e90-9fac-29967bb6eef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T17:05:41.041668Z",
     "iopub.status.busy": "2023-04-06T17:05:41.038512Z",
     "iopub.status.idle": "2023-04-06T17:05:41.063552Z",
     "shell.execute_reply": "2023-04-06T17:05:41.057103Z",
     "shell.execute_reply.started": "2023-04-06T17:05:41.041620Z"
    },
    "tags": []
   },
   "source": [
    "Toy code to split domain names into subdomains\n",
    "# Regex pattern to extract domain name\n",
    "pattern = r\"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?(?P<domain>[\\w\\.\\-]+)[^\\w]*\"\n",
    "\n",
    "url = pd.DataFrame([\"www.test.com\", \"www.api.test2.com\"], columns=[\"url\"])\n",
    "\n",
    "match = url[\"url\"].str.extract(pattern)\n",
    "\n",
    "pd.DataFrame(match[\"domain\"].apply(lambda x : x.split('.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293234be-c806-4062-b94d-e970b04b4d99",
   "metadata": {},
   "source": [
    "## Alternative library for tld matching"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a05c36b-03a6-4293-b581-a34ffc0ff02e",
   "metadata": {},
   "source": [
    "May be faster, too much pain for no gain\n",
    "import tld\n",
    "# Function to extract domain components using tld\n",
    "def extract_domain_components_tld(url):\n",
    "    try:\n",
    "        res = tld.get_tld(url, as_object=True, fix_protocol=True)\n",
    "        return pd.Series([res.subdomain, res.domain, res.tld, res.tld == \"\"])\n",
    "    except tld.exceptions.TldDomainNotFound:\n",
    "        return pd.Series([\"\", url, \"\", True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa4bc5-097e-4ba6-8012-6c861e554fc7",
   "metadata": {},
   "source": [
    "#### N-grams"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a4de825-bcff-4e3b-a8c9-7e8fdec2306b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T12:38:05.679608Z",
     "iopub.status.busy": "2023-04-19T12:38:05.676314Z",
     "iopub.status.idle": "2023-04-19T12:38:36.750529Z",
     "shell.execute_reply": "2023-04-19T12:38:36.749490Z",
     "shell.execute_reply.started": "2023-04-19T12:38:05.679545Z"
    },
    "tags": []
   },
   "source": [
    "# Define columns to compute Ngrams over\n",
    "NGRAM_COLUMNS = [\"domain\", \"subdomain\"]\n",
    "\n",
    "# Ngrams implementation over characters\n",
    "def ngrams(string, n):\n",
    "    return [\"\".join(string[i : i + n]) for i in range(len(string) - n + 1)]\n",
    "\n",
    "\n",
    "# Define values for n\n",
    "n_values = [2, 3]\n",
    "\n",
    "for n in n_values:\n",
    "    # Create names or new columns\n",
    "    ngram_new_columns = [f\"{col}_n{n}grams\" for col in NGRAM_COLUMNS]\n",
    "\n",
    "    # Apply vectorized function over dataframe\n",
    "    urls_df[ngram_new_columns] = urls_df[NGRAM_COLUMNS].applymap(lambda x: ngrams(x, n))\n",
    "\n",
    "urls_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f0593-23d6-4a71-99b5-ade9e056dfec",
   "metadata": {},
   "source": [
    "#### Non-functional version of TF-IDF\n",
    "\n",
    "Crashes on the .toarray() function\n",
    "Might be memory issue?\n",
    "\n",
    "TfidfVectorizer works though"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1de15ae-df69-4893-ae74-e8bff02babdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Define n-gram range\n",
    "ngram_range = (1, 3)\n",
    "\n",
    "# Compute TF-IDF features for 'domain' column\n",
    "cv_domain = CountVectorizer(ngram_range=ngram_range, analyzer=\"char\")\n",
    "\n",
    "tfidf_transformer_domain = TfidfTransformer()\n",
    "\n",
    "X_domain = cv_domain.fit_transform(urls_df[\"domain\"])\n",
    "\n",
    "X_domain_tfidf = tfidf_transformer_domain.fit_transform(X_domain)\n",
    "\n",
    "# Create a pandas DataFrame for the TF-IDF features of 'domain' column\n",
    "# tfidf_domain_df = pd.DataFrame(X_domain_tfidf.toarray(), columns=cv_domain.get_feature_names_out())\n",
    "\n",
    "# Compute TF-IDF features for 'subdomain' column\n",
    "cv_subdomain = CountVectorizer(ngram_range=ngram_range, analyzer=\"char\")\n",
    "\n",
    "tfidf_transformer_subdomain = TfidfTransformer()\n",
    "\n",
    "X_subdomain = cv_subdomain.fit_transform(urls_df[\"subdomain\"])\n",
    "\n",
    "X_subdomain_tfidf = tfidf_transformer_subdomain.fit_transform(X_subdomain)\n",
    "\n",
    "X_domain_tfidf.toarray()\n",
    "\n",
    "# Create a pandas DataFrame for the TF-IDF features of 'subdomain' column\n",
    "# tfidf_subdomain_df = pd.DataFrame(X_subdomain_tfidf.toarray(), columns=cv_subdomain.get_feature_names_out())\n",
    "\n",
    "# Combine the TF-IDF features for both columns into a single DataFrame\n",
    "# tfidf_df = pd.concat([tfidf_domain_df, tfidf_subdomain_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca4e45-fe23-4067-9545-df57a8fb5181",
   "metadata": {},
   "source": [
    "## TF-IDF using HashingVectorizer\n",
    "\n",
    "Not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97863403-a34e-4117-98ec-e17dfaae008d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-27T17:11:32.460247Z",
     "iopub.status.idle": "2023-04-27T17:11:32.461481Z",
     "shell.execute_reply": "2023-04-27T17:11:32.461146Z",
     "shell.execute_reply.started": "2023-04-27T17:11:32.461108Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "class IncrementalHashingVectorizer:\n",
    "    def __init__(self, n_features=2**18, ngram_range=(1, 1), **kwargs):\n",
    "        self.vectorizer = HashingVectorizer(\n",
    "            n_features=n_features, ngram_range=ngram_range, **kwargs\n",
    "        )\n",
    "        self.partial_fit_calls = 0\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        if self.partial_fit_calls == 0:\n",
    "            self.features_sum = np.zeros(self.vectorizer.n_features)\n",
    "            self.feature_counts = np.zeros(self.vectorizer.n_features)\n",
    "\n",
    "        transformed = self.vectorizer.transform(X)\n",
    "        self.features_sum += transformed.sum(axis=0)\n",
    "        self.feature_counts += transformed.getnnz(axis=0)\n",
    "        self.partial_fit_calls += 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed = self.vectorizer.transform(X)\n",
    "        idf_vector = np.log(self.partial_fit_calls / (1 + self.feature_counts)) + 1\n",
    "        return transformed.multiply(idf_vector)\n",
    "\n",
    "\n",
    "incremental_vectorizer = IncrementalHashingVectorizer(\n",
    "    ngram_range=NGRAM_RANGE, analyzer=\"char\"\n",
    ")\n",
    "\n",
    "\n",
    "incremental_vectorizer.partial_fit([doc])\n",
    "\n",
    "transformed_documents = incremental_vectorizer.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896d5b5-c3c7-40bf-99df-64188ef2872e",
   "metadata": {},
   "source": [
    "## TF-IDF on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a85b8-67d0-4362-b4c4-ef79eb15b595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T17:14:02.262529Z",
     "iopub.status.busy": "2023-04-27T17:14:02.259240Z",
     "iopub.status.idle": "2023-04-27T17:15:03.464503Z",
     "shell.execute_reply": "2023-04-27T17:15:03.434933Z",
     "shell.execute_reply.started": "2023-04-27T17:14:02.262459Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer instance for domain column\n",
    "tfidf_domain = TfidfVectorizer(analyzer=\"char\", ngram_range=NGRAM_RANGE)\n",
    "# Fit and transform domain column\n",
    "tfidf_domain_matrix = tfidf_domain.fit_transform(urls_df[\"domain\"])\n",
    "\n",
    "# Create TfidfVectorizer instance for subdomain column\n",
    "tfidf_subdomain = TfidfVectorizer(analyzer=\"char\", ngram_range=NGRAM_RANGE)\n",
    "# Fit and transform subdomain column\n",
    "tfidf_subdomain_matrix = tfidf_subdomain.fit_transform(urls_df[\"subdomain\"])\n",
    "\n",
    "# Create pandas dataframe from tfidf_matrix\n",
    "tfidf_domain_df_tfidfv = pd.DataFrame(\n",
    "    tfidf_domain_matrix.toarray(),\n",
    "    columns=tfidf_domain.get_feature_names_out(),\n",
    "    index=urls_df.index,\n",
    ")\n",
    "tfidf_subdomain_df_tfidfv = pd.DataFrame(\n",
    "    tfidf_subdomain_matrix.toarray(),\n",
    "    columns=tfidf_subdomain.get_feature_names_out(),\n",
    "    index=urls_df.index,\n",
    ")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "# tfidf_df = pd.concat([tfidf_domain_df, tfidf_subdomain_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10256b6-5deb-4173-a5df-dca319ea3161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T17:15:03.516338Z",
     "iopub.status.busy": "2023-04-27T17:15:03.513464Z",
     "iopub.status.idle": "2023-04-27T17:15:16.918345Z",
     "shell.execute_reply": "2023-04-27T17:15:16.916741Z",
     "shell.execute_reply.started": "2023-04-27T17:15:03.516211Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 638971 entries, 0 to 638970\n",
      "Columns: 165105 entries, '01 to zzzi\n",
      "dtypes: float64(165105)\n",
      "memory usage: 786.0 GB\n"
     ]
    }
   ],
   "source": [
    "tfidf_domain_df_tfidfv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714e145-c962-4080-8519-68b0b31b3e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T17:15:16.964210Z",
     "iopub.status.busy": "2023-04-27T17:15:16.963175Z",
     "iopub.status.idle": "2023-04-27T17:15:35.073809Z",
     "shell.execute_reply": "2023-04-27T17:15:35.070903Z",
     "shell.execute_reply.started": "2023-04-27T17:15:16.964172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 638971 entries, 0 to 638970\n",
      "Columns: 162855 entries, '00 to zzzz\n",
      "dtypes: float64(162855)\n",
      "memory usage: 775.3 GB\n"
     ]
    }
   ],
   "source": [
    "tfidf_subdomain_df_tfidfv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f60d53-4bec-42a4-bdf8-d68cfb2ca033",
   "metadata": {},
   "source": [
    "## How to use LSA for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27eb6158-c22e-4540-be89-af074a5aeecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T09:00:17.552165Z",
     "iopub.status.busy": "2023-04-28T09:00:17.551430Z",
     "iopub.status.idle": "2023-04-28T09:00:32.639299Z",
     "shell.execute_reply": "2023-04-28T09:00:32.637484Z",
     "shell.execute_reply.started": "2023-04-28T09:00:17.552121Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 49.2%\n"
     ]
    }
   ],
   "source": [
    "# Taken from https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#performing-dimensionality-reduction-using-lsa\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=10), Normalizer(copy=False))\n",
    "\n",
    "X_domain_lsa = lsa.fit_transform(X_domain_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelor thesis kernel",
   "language": "python",
   "name": "avast_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
