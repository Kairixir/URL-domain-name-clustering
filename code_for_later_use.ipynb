{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bcdbb2-e52e-478b-8b60-46f841aa3565",
   "metadata": {},
   "source": [
    "# Code that might be useful later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dec22e-17cb-4839-bd9f-90baf3d9204c",
   "metadata": {},
   "source": [
    "## Sklearn trick for clustering strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8899a20d-3098-4852-842c-f8f03e9e9392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:24:27.862539Z",
     "iopub.status.busy": "2023-04-14T18:24:27.861251Z",
     "iopub.status.idle": "2023-04-14T18:24:27.923930Z",
     "shell.execute_reply": "2023-04-14T18:24:27.895845Z",
     "shell.execute_reply.started": "2023-04-14T18:24:27.862498Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SklearnStringTrick:\n",
    "    \"\"\"\n",
    "    String clustering trick shown at:\n",
    "        https://scikit-learn.org/0.16/faq.html#how-do-i-deal-with-string-data-or-trees-graphs\n",
    "\n",
    "    Class structure allows for method reuse over distinct data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset=[]):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def lev_dist_sklearn_urls(self, x, y):\n",
    "        i, j = int(x[0]), int(y[0])  # extract indices\n",
    "        return levenshtein(self.dataset[i], self.dataset[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774571e8-f941-4ec4-8dd9-f870fe608c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:24:27.942634Z",
     "iopub.status.busy": "2023-04-14T18:24:27.939522Z",
     "iopub.status.idle": "2023-04-14T18:24:27.962753Z",
     "shell.execute_reply": "2023-04-14T18:24:27.957185Z",
     "shell.execute_reply.started": "2023-04-14T18:24:27.942554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_trick = SklearnStringTrick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f9c98-c409-401b-acc4-eda6fe3ba5ea",
   "metadata": {},
   "source": [
    "## Pattern for filtering IPv6 addresses"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bea7d64a-3c9f-47a7-a363-8046ad092d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:15:14.871410Z",
     "iopub.status.busy": "2023-04-14T18:15:14.870172Z",
     "iopub.status.idle": "2023-04-14T18:15:16.927432Z",
     "shell.execute_reply": "2023-04-14T18:15:16.926093Z",
     "shell.execute_reply.started": "2023-04-14T18:15:14.871354Z"
    },
    "tags": []
   },
   "source": [
    "# Pattern that matches all IPv6 addresses (taken from https://stackoverflow.com/a/17871737\n",
    "pattern = \"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\"\n",
    "urls_df[urls_df[\"url\"].str.match(pattern)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb629117-8e96-4ada-8734-e90b94976444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T18:16:40.691981Z",
     "iopub.status.busy": "2023-04-14T18:16:40.691617Z",
     "iopub.status.idle": "2023-04-14T18:16:42.286440Z",
     "shell.execute_reply": "2023-04-14T18:16:42.283345Z",
     "shell.execute_reply.started": "2023-04-14T18:16:40.691944Z"
    },
    "tags": []
   },
   "source": [
    "# To check remaining URLs\n",
    "urls_df[urls_df[\"url\"].str.match(\"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29d867-0eb1-4551-b579-83684d41dd7a",
   "metadata": {},
   "source": [
    "## My attempt for regex splitting domain and subdomains"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5bdae30-cd76-4e90-9fac-29967bb6eef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T17:05:41.041668Z",
     "iopub.status.busy": "2023-04-06T17:05:41.038512Z",
     "iopub.status.idle": "2023-04-06T17:05:41.063552Z",
     "shell.execute_reply": "2023-04-06T17:05:41.057103Z",
     "shell.execute_reply.started": "2023-04-06T17:05:41.041620Z"
    },
    "tags": []
   },
   "source": [
    "Toy code to split domain names into subdomains\n",
    "# Regex pattern to extract domain name\n",
    "pattern = r\"(?:.*?:\\/\\/)?(?P<www>[wW]{3}\\.)?(?P<domain>[\\w\\.\\-]+)[^\\w]*\"\n",
    "\n",
    "url = pd.DataFrame([\"www.test.com\", \"www.api.test2.com\"], columns=[\"url\"])\n",
    "\n",
    "match = url[\"url\"].str.extract(pattern)\n",
    "\n",
    "pd.DataFrame(match[\"domain\"].apply(lambda x : x.split('.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293234be-c806-4062-b94d-e970b04b4d99",
   "metadata": {},
   "source": [
    "## Alternative library for tld matching"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a05c36b-03a6-4293-b581-a34ffc0ff02e",
   "metadata": {},
   "source": [
    "May be faster, too much pain for no gain\n",
    "import tld\n",
    "# Function to extract domain components using tld\n",
    "def extract_domain_components_tld(url):\n",
    "    try:\n",
    "        res = tld.get_tld(url, as_object=True, fix_protocol=True)\n",
    "        return pd.Series([res.subdomain, res.domain, res.tld, res.tld == \"\"])\n",
    "    except tld.exceptions.TldDomainNotFound:\n",
    "        return pd.Series([\"\", url, \"\", True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa4bc5-097e-4ba6-8012-6c861e554fc7",
   "metadata": {},
   "source": [
    "#### N-grams"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a4de825-bcff-4e3b-a8c9-7e8fdec2306b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T12:38:05.679608Z",
     "iopub.status.busy": "2023-04-19T12:38:05.676314Z",
     "iopub.status.idle": "2023-04-19T12:38:36.750529Z",
     "shell.execute_reply": "2023-04-19T12:38:36.749490Z",
     "shell.execute_reply.started": "2023-04-19T12:38:05.679545Z"
    },
    "tags": []
   },
   "source": [
    "# Define columns to compute Ngrams over\n",
    "NGRAM_COLUMNS = [\"domain\", \"subdomain\"]\n",
    "\n",
    "# Ngrams implementation over characters\n",
    "def ngrams(string, n):\n",
    "    return [\"\".join(string[i : i + n]) for i in range(len(string) - n + 1)]\n",
    "\n",
    "\n",
    "# Define values for n\n",
    "n_values = [2, 3]\n",
    "\n",
    "for n in n_values:\n",
    "    # Create names or new columns\n",
    "    ngram_new_columns = [f\"{col}_n{n}grams\" for col in NGRAM_COLUMNS]\n",
    "\n",
    "    # Apply vectorized function over dataframe\n",
    "    urls_df[ngram_new_columns] = urls_df[NGRAM_COLUMNS].applymap(lambda x: ngrams(x, n))\n",
    "\n",
    "urls_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f0593-23d6-4a71-99b5-ade9e056dfec",
   "metadata": {},
   "source": [
    "#### Non-functional version of TF-IDF\n",
    "\n",
    "Crashes on the .toarray() function\n",
    "Might be memory issue?\n",
    "\n",
    "TfidfVectorizer works though"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1de15ae-df69-4893-ae74-e8bff02babdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Define n-gram range\n",
    "ngram_range = (1, 3)\n",
    "\n",
    "# Compute TF-IDF features for 'domain' column\n",
    "cv_domain = CountVectorizer(ngram_range=ngram_range, analyzer=\"char\")\n",
    "\n",
    "tfidf_transformer_domain = TfidfTransformer()\n",
    "\n",
    "X_domain = cv_domain.fit_transform(urls_df[\"domain\"])\n",
    "\n",
    "X_domain_tfidf = tfidf_transformer_domain.fit_transform(X_domain)\n",
    "\n",
    "# Create a pandas DataFrame for the TF-IDF features of 'domain' column\n",
    "# tfidf_domain_df = pd.DataFrame(X_domain_tfidf.toarray(), columns=cv_domain.get_feature_names_out())\n",
    "\n",
    "# Compute TF-IDF features for 'subdomain' column\n",
    "cv_subdomain = CountVectorizer(ngram_range=ngram_range, analyzer=\"char\")\n",
    "\n",
    "tfidf_transformer_subdomain = TfidfTransformer()\n",
    "\n",
    "X_subdomain = cv_subdomain.fit_transform(urls_df[\"subdomain\"])\n",
    "\n",
    "X_subdomain_tfidf = tfidf_transformer_subdomain.fit_transform(X_subdomain)\n",
    "\n",
    "X_domain_tfidf.toarray()\n",
    "\n",
    "# Create a pandas DataFrame for the TF-IDF features of 'subdomain' column\n",
    "# tfidf_subdomain_df = pd.DataFrame(X_subdomain_tfidf.toarray(), columns=cv_subdomain.get_feature_names_out())\n",
    "\n",
    "# Combine the TF-IDF features for both columns into a single DataFrame\n",
    "# tfidf_df = pd.concat([tfidf_domain_df, tfidf_subdomain_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelor thesis kernel",
   "language": "python",
   "name": "avast_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
